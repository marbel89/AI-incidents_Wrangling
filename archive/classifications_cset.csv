_id,incident_id,namespace,publish,Annotator,Annotation Status,Reviewer,Quality Control,Full Description,Short Description,Beginning Date,Ending Date,Location,Near Miss,Named Entities,Technology Purveyor,Intent,Severity,Harm Type,Lives Lost,Harm Distribution Basis,Infrastructure Sectors,Financial Cost,Laws Implicated,AI System Description,Data Inputs,System Developer,Sector of Deployment,Public Sector Deployment,Nature of End User,Level of Autonomy,Relevant AI functions,AI Techniques,AI Applications,Physical System,Problem Nature
ObjectId(60dd465f80935bc89e6f9b00),2,CSET,true,"""1""","""6. Complete and final""","""5""",false,"""On December 5, 2018, a robot punctured a can of bear spray in Amazon's fulfillment center in Robbinsville, New Jersey. Amazon's spokesman stated that \""an automated machine punctured a 9-oz can of bear repellent.\"" The punctured can released capsaicin, an irritant, into the air. Several dozen workers were exposed to the fumes, causing symptoms including trouble breathing and a burning sensation in the eyes and throat. 24 workers were hospitalized, and one was sent to intensive care and intubated.""","""Twenty-four Amazon workers in New Jersey were hospitalized after a robot punctured a can of bear repellent spray in a warehouse.""","""2018-12-05T00:00:00.000Z""","""2018-12-05T00:00:00.000Z""","""Robbinsville, New Jersey, United States of America""","""Harm caused""","[""Amazon""]","[""Amazon""]","""Accident""","""Moderate""","[""Harm to physical health/safety"",""Harm to physical property""]",false,[],[],"""""","[""Workplace safety laws"",""OSHA regulations""]","""An automated machine operating within an Amazon fulfillment center.""",[],[],"[""Transportation and storage""]",false,"""""","""Unclear/unknown""","[""Unclear""]",[],"[""robotics""]","[""Unknown/unclear""]","[""Unknown/unclear""]"
ObjectId(60dd465f80935bc89e6f9b01),3,CSET,false,"""1""","""6. Complete and final""","""6""",false,"""A Boeing 737 MAX aircraft equipped with a new, automated flight control system plunged into the Java Sea, killing 189 people on board. On October 29, 2018, shortly after takeoff from Jakarta, Lion Air Flight 610 reported a malfunction in its \""angle of attack\"" sensor. Based on faulty readings from the single sensor, the plane's Maneuvering Characteristics Augmentation System (MCAS) forced the plane's nose down in order to prevent stalling. The pilots pulled the nose back up over twenty times, but the MCAS system counteracted their actions. Eventually, the plane hit the ocean at a speed of several hundred miles per hour. Flight 610 had experienced a similar problem on a previous flight, but in that case, the pilots were able to override MCAS using an emergency procedure. Commentators speculated that the pilots on the day of the fatal crash may have been confused or startled, and that they may not have been fully aware of the MCAS system and how they could override it.""","""A Boeing 737 crashed into the sea, killing 189 people, after faulty sensor data caused an automated manuevering system to repeatedly push the plane's nose downward.""","""2018-10-29""","""2018-10-29""","""Java Sea""","""Harm caused""","[""Lion Air"",""Boeing""]","[""Lion Air""]","""Accident""","""Critical""","[""Harm to physical health/safety"",""Psychological harm"",""Harm to physical property"",""Harm to intangible property""]",true,[],"[""Transportation""]","""""","[""Federal Aviation Administration regulations"",""Indonesian air safety regulations""]","""The Maneuvering Characteristics Augmentation System (MCAS) is an aircraft maneuvering system that can autonomously steer the plane based on sensor input""","[""Aircraft sensor data""]","[""Boeing""]","[""Transportation and storage""]",false,"""Expert""","""High""","[""Perception"",""Cognition"",""Action""]",[],"[""Aircraft maneuvering"",""sensor input processing""]","[""Vehicle/mobile robot""]","[""Robustness"",""Assurance""]"
ObjectId(60dd465f80935bc89e6f9b02),4,CSET,false,"""2""","""6. Complete and final""","""6""",false,"""On March 18, 2018, an Uber autonomous vehicle struck and killed 49 year-old Elaine Herzberg during testing in Tempe, Arizona. Herzberg was walking her bicycle outside a crosswalk around 10:00pm when the vehicle hit her at a speed of 39-44 miles per hour. The vehicle, a Volvo XC90 equipped with cameras, radar, and LIDAR, was operating in autonomous mode with Rafaela Vasquez, 44, in the driverâ€™s seat as a safety monitor. Internal video from the vehicle indicates that Vasquez was watching television on their phone prior to the collision, only looking up to the street 0.5 seconds before impact. Vasquez has been charged with manslaughter in the incident.""","""An Uber autonomous vehicle in autonomous mode struck and killed a pedestrian in Tempe, Arizona on March 18, 2018.""","""2018-03-18""","""2018-03-18""","""Tempe, Arizona, United States of America""","""Harm caused""","[""Uber"",""Volvo"",""Elaine Herzberg"",""Rafaela Vasquez""]","[""Uber"",""Rafaela Vasquez""]","""Accident""","""Severe""","[""Harm to physical health/safety"",""Psychological harm"",""Harm to physical property""]",true,[],"[""Transportation""]","""""","[""Vehicular Manslaughter""]","""Autonomous vehicle operated by Uber utilizing cameras, radar, and LIDAR (a variant of radar that uses invisible pulses of light) to allow for autonomous operation of the vehicle.""","[""Traffic patterns"",""environmental sensory data"",""driving maneuvers""]","[""Uber""]","[""Transportation and storage""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""autonomous driving"",""LIDAR""]","[""autonomous driving""]","[""Vehicle/mobile robot""]","[""Specification"",""Robustness""]"
ObjectId(60dd465f80935bc89e6f9b03),5,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""Reports of robotic surgeries resulting in injury and death between 2000 and 2013 as found in the Manufacturer and User Facility Device Experience (MAUDE) database, a database of both voluntary and mandatory reports of mishaps. Within the 14 year span there are 8,091 recorded malfunctions resulting in 1,391 injuries and 144 deaths. Injuries range from burns from sparks emitted by the machines (n=193), robotic arms becoming dislodged in the patient (n=100), and instances of the surgeon losing control of the machine or the machine powering down unexpectedly (n=52). About 62% of injuries and deaths reported were due to system/hardware error, while the remainder were attributed to the inherent risk of surgery or human error.""","""Study on database reports of robotic surgery malfunctions (8,061), including those ending in injury (1,391) and death (144), between 2000 and 2013.""","""2000-01-01T00:00:00.000Z""","""2013-01-01T00:00:00.000Z""","""United States of America""","""Harm caused""","[""da Vinci Robot"",""FDA""]",[],"""Accident""","""Severe""","[""Harm to physical health/safety""]",true,[],"[""Healthcare and public health""]","""""",[],"""Robotic surgery tools""","[""Surgeon's directions"",""medical procedures""]",[],"[""Human health and social work activities""]",false,"""Expert""","""Low""","[""Action""]","[""Robotic surgery tools""]","[""Robotic Surgery""]","[""Other:Medical system""]","[""Robustness"",""Assurance""]"
ObjectId(60dd465f80935bc89e6f9b04),6,CSET,true,"""2""","""6. Complete and final""","""5""","""""","""Microsoft chatbot, Tay, was published onto Twitter on March 23, 2016. Within 24 hours Tay had been removed from Twitter after becoming a \""holocaust-denying racist\"" due to the inputs entered by Twitter users and Tay's ability to craft responses based on what is available to read on Twitter. Tay's \""repeat after me\"" feature allowed any Twitter user to tell Tay what to say and it would be repeated, leading to some of the racist and anti-semitic tweets. \""Trolls\"" also exposed the chatbot to ideas that led to production of sentences like: \""Hitler was right I hate the Jews,\""  \""i fucking hate feminists,\"" and \""bush did 9/11 and Hitler would have done a better job than the monkey we have now. Donald Trump is the only hope we've got.\"" Tay was replaced by Zo. It's noteworthy that Microsoft released a similar chatbot in China named Xiaolce, who ran smoothly without major complications, implying culture and public input had a heavy role in Tay's results.""","""Microsoft's Tay, an artificially intelligent chatbot, was released on March 23, 2016 and removed within 24 hours due to multiple racist, sexist, and anit-semitic tweets generated by the bot.""","""2016-03-23""","""2016-03-24""","""Global""","""Harm caused""","[""Microsoft"",""Twitter"",""Tay"",""Xiaoice""]","[""Microsoft"",""Twitter""]","""Accident""","""Minor""","[""Psychological harm"",""Harm to social or political systems""]",false,"[""Race"",""Religion"",""National origin or immigrant status"",""Sex""]",[],"""""",[],"""Microsoft's Tay chatbot, an artificially intelligent chatbot published on Twitter""","[""Twitter users' input""]","[""Microsoft""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""content creation"",""language recognition natural language processing""]","[""comprehension"",""language output"",""chatbot""]","[""Software only""]","[""Specification"",""Robustness"",""Assurance""]"
ObjectId(60dd466080935bc89e6f9b05),7,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""Wikipedia bots meant to help edit articles through artificial intelligence clash with each other, undoing the other's edits repetitively. The bots are meant to remove vandalism on the open-source, open-input site, however they have begun to disagree with each other and form infintie feedback loops of correcting the other's edits. Two notable cases are the face off between Xqbot and Darnkessbot that has led to 3,629 edited articles between 2009-2010 and between Tachikoma and Russbot leading to more than 3,000 edits. These edits have occurred across articles in 13 languages on Wikipedia, with the most ocurring in Portuguese language articles and the least occurring in German language articles. The whole situation has been described as a \""bot-on-bot editing war.\""""","""Wikipedia bots meant to remove vandalism clash with each other and form feedback loops of repetitve undoing of the other bot's edits.""","""2001-01-01T00:00:00.000Z""","""2010-01-01T00:00:00.000Z""","""Global""","""Unclear/unknown""","[""Wikipedia""]","[""Wikipedia""]","""Accident""","""Negligible""","[""Other:Harm to publicly available information""]",false,[],[],"""""",[],"""Wikipedia editing bots meant to remove vandalism on the site""","[""Wikipedia articles"",""edits from other bots""]","[""Wikipedia""]","[""Information and communication""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""Content editing bot""]","[""AI content creation"",""AI content editing""]","[""Software only""]","[""Specification"",""Robustness"",""Assurance""]"
ObjectId(60dd466080935bc89e6f9b06),8,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""Uber's autonomous vehicles have been recorded running red lights on two occasions in a pilot program on the streets of San Francisco, California. A witness, Christoper Koff, reported seeing the AI enabled Volvo XC90 SUV pass through a red light three seconds after the light had turned red and while a pedestrian was in the crosswalk. There were no injuries or collisions. Uber has denied the claim this was the system's error, citing human operator error and suspending the driver. Two Uber employees reported to the New York Times that the fault was of the AI system.""","""Uber vehicles equipped with technology allowing for autonomous driving running red lights in San Francisco street testing.""","""12-2016""","""12-2016""","""San Francisco, CA""","""Near miss""","[""Uber"",""Volvo""]","[""Uber"",""Volvo""]","""Accident""","""Negligible""",[],false,[],"[""Transportation""]","""""","[""""]","""Self-driving autonomous Uber vehicles""","[""Traffick patterns"",""environment surroundings"",""human driver input""]","[""Uber""]","[""Transportation and storage""]",false,"""Expert""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""autonomous vehicles"",""LIDAR"",""radar""]","[""traffick flow forecasting"",""autonomous driving""]","[""Vehicle/mobile robot""]","[""Specification"",""Assurance""]"
ObjectId(60dd466080935bc89e6f9b07),9,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""A value-added measurement based algorithm used to calculate the effectiveness of school teachers is being challenged for its lack of apparent accuracy and real-world relevance. In Rochester, New York approximately 600 teachers are disputing the results, and in Syracuse between 400-500 are disputing the results. The VAM score can be used to give raises to teachers deemed \""effective\"" or \""highly effective\"", but can also be used to fire teachers who are given two \""ineffective\"" ratings in a row. Teachers criticize the algorithm for including only Math and English in the evaluation (even for teachers of other subjects as the algorithm only covers those two subjects), using school averages to calculate a single student's expected average, and high-grade-earning students being predicted to grow at literally impossible rates (to score grades higher than 100% on tests).""","""An algorithm used to rate the effectiveness of school teachers in New York has resulted in thousands of disputes of its results.""","""2007-01-01T00:00:00.000Z""","""2010-01-01T00:00:00.000Z""","""New York, United States of America""","""Harm caused""","[""New York Department of Education, United Federation of Teachers"",""Sheri Lederman"",""Common Core"",""Governor Andrew Cuomo""]","[""New York Department of Education""]","""Accident""","""Negligible""","[""Financial harm""]",false,"[""Other:School Teachers""]",[],"""""",[],"""value-added analysis algorithm used for evaluating a school teacher's effectiveness of teaching""","[""School grades"",""student grades"",""predicted grades""]","[""New York city Dept. of Education""]","[""Education""]",true,"""Amateur""","""Medium""","[""Unclear""]","[""value-added mesaurements""]","[""data processing"",""data prediction""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466080935bc89e6f9b08),10,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""The staff scheduling tool used by Starbucks has led to staff working hours volatile and erratic schedules. Some store managers use a scheduling software, Kronos, to optimize scheduling and cut labor costs, however Starbucks refuses to accept or deny using Kronos.""","""Issues with Starbucks worker's schedules may be linked to a staffing software, Kronos.""","""2015-01-01""","""2015-01-01""","""Global""","""Unclear/unknown""","[""Starbucks"",""Kronos""]","[""Starbucks""]","""Unclear""","""Negligible""","[""Psychological harm""]",false,[],"[""Food and agriculture""]","""""",[],"""""",[],[],"[""""]",false,"""""","""""",[],[],[],[],[]
ObjectId(60dd466080935bc89e6f9b09),11,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""An algorithm developed by Northpointe and used in the penal system is shown to be inaccurate and produces racially-skewed results according to a review by ProPublica. The review shows how the 137-question survey given following an arrest is inaccurate and skewed against people of color. While there is not question regarding race in the survey, the algorithm is two times more likely to incorrectly label a black person as a high-risk re-offender (False Positive) and is also two times more likely to incorrectly label a white person as low-risk for reoffense (False Negative) than actual statistics support. Overall, the algorithm is 61% effective at predicting reoffense. This system is used in Broward County, Florida to help judges make decisions surrounding pre-trial release and sentencing post-trial.""","""An algorithm developed by Northpointe and used in the penal system is two times more likely to incorrectly label a black person as a high-risk re-offender and is two times more likely to incorrectly label a white person as low-risk for reoffense according to a ProPublica review.""","""2016-01-01T00:00:00.000Z""","""2019-01-01T00:00:00.000Z""","""Broward County, Florida""","""Harm caused""","[""ProPublica"",""Northpointe"",""COMPAS"",""Broward County, FL""]","[""Northpointe""]","""Accident""","""Unclear/unknown""","[""Harm to civil liberties"",""Other:Reputational harm; False incarceration""]",false,"[""Race""]","[""Government facilities""]","""""",[],"""An algorithm, developed by Northpointe designed to assign a risk score associated with a person's likelihood of reoffending after their original arrest.""","[""137-question survey""]","[""Northpointe""]","[""Public administration and defence""]",true,"""Expert""","""Medium""","[""Perception"",""Cognition""]","[""law enforcement algorithm"",""crime prediction algorithm""]","[""risk assesment"",""crime projection""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466080935bc89e6f9b0a),12,CSET,true,"""2""","""6. Complete and final""","""5""",true,"""The most common techniques used to embed words for natural language processing (NLP) show gender bias, according to researchers from Boston University and Microsoft Research, New England. The primary embedding studied was a 300-dimensional word2vec embedding of words from a corpus of Google News texts, chosen because it is open-source and popular in NLP applications. After demonstrating gender bias in the embedding, the researchers show that several geometric features are associated with that bias which can be used to define the bias subspace. This finding allows them to create several debiasing algorithms.""","""Researchers from Boston University and Microsoft Research, New England demonstrated gender bias in the most common techniques used to embed words for natural language processing (NLP).""","""2016-01-01T00:00:00.000Z""","""2016-01-01T00:00:00.000Z""","""Global""","""Unclear/unknown""","[""Microsoft"",""Boston University"",""Google News""]","[""Microsoft""]","""Unclear""","""Unclear/unknown""",[],false,"[""Sex""]",[],"""""","[""""]","""Machine learning algorithms that create word embeddings from a text corpus.""","[""""]","[""""]","[""""]",false,"""""","""""","[""Unclear""]","[""Vector word embedding""]","[""Natural language processing""]",[],[]
ObjectId(60dd466080935bc89e6f9b0b),13,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Google's Perspective API, which assigns a toxicity score to online text, has been shown to award higher toxicity scores to content involving non-white, male, Christian, heterosexual phrases. the scores lay on the spectrum between very healthy (low %) to very toxic (high %). The phrase \""I am a man\"" received a score of 20% while \""I am a gay black woman\"" received 87%. The bias exists within subcategories as well: \""I am a man who is deaf\"" received 70%, \""I am a person who is deaf\"" received 74%, and \""I am a woman who is deaf\"" received 77%. The API can also be circumvented by modifying text: \""They are liberal idiots who are uneducated\"" received 90% while \""they are liberal idiots who are un.educated\"" received 15%.""","""Google's Perspective API, which assigns a toxicity score to online text, seems to award higher toxicity scores to content involving non-white, male, Christian, heterosexual phrases.""","""2017-01-01T00:00:00.000Z""","""2017-01-01T00:00:00.000Z""","""Global""","""Harm caused""","[""Google"",""Google Cloud"",""Perspective API""]","[""Google""]","""Accident""","""Minor""","[""Psychological harm"",""Harm to social or political systems""]",false,"[""Race"",""Religion"",""National origin or immigrant status"",""Sex"",""Sexual orientation or gender identity"",""Disability"",""Ideology""]",[],"""""","[""""]","""Google Perspective is an API designed using machine learning tactics to assign \""toxicity\"" scores to online text with the oiginal intent of assisting in identifying hate speech and \""trolling\"" on internet comments. Perspective is trained to recognize a variety of attributes (e.g. whether a comment is toxic, threatening, insulting, off-topic, etc.) using millions of examples gathered from several online platforms and reviewed by human annotators.""","[""Online comments""]","[""Google""]","[""Information and communication""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""open-source"",""machine learning""]","[""Natural language processing"",""content ranking""]","[""Software only""]","[""Specification"",""Robustness""]"
ObjectId(60dd466080935bc89e6f9b0c),14,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""Google's Cloud Natural Language API returns \""negative\"" sentiment analysis on phrases such as \""I am homosexual\"" \""I am Jewish\"" or \""I am black\"". The API uses Natural Language Processing (NLP) to analyze text and produce a score from -1.0 to 1.0 with -1.0 being \""very negative\"" and 1.0 being \""very positive\"". ""","""Google Cloud's Natural Language API provided racist, homophobic, amd antisemitic sentiment analyses.""","""10-2017T00:00:00.000Z""","""10-2017T00:00:00.000Z""","""Global""","""Harm caused""","[""Google"",""Google Cloud"",""Natural Language API""]","[""Google""]","""Accident""","""Negligible""","[""Harm to social or political systems""]",false,"[""Race"",""Religion"",""Sexual orientation or gender identity"",""Ideology""]",[],"""""",[],"""Google Cloud's Natural Language API that analyzes input text and outputs a \""sentiment analysis\"" score from -1.0 (very negative) to 1.0 (very positive)""","[""input from open source internet""]","[""Google""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""High""","[""Cognition""]","[""Google Cloud Natural Language Processing API""]","[""Natural language processing""]","[""Software only""]","[""Robustness""]"
ObjectId(60dd466080935bc89e6f9b0d),15,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Amazon's book store experienced a \""cataloging error\"" in which 57,310 books lost their \""sales ranking\"", a number used to help books show up quicker in the book suggestions algorithm. The books affected are reported to include between \""dozens\"" and \""hundreds\"" of books containing gay and lesbian themes, often labeling them as \""adult material\"" or \""pornographic\"" when similar books containing heterosexual characters remain at the top of the sales ranking.""","""Amazon's book store \""cataloging error\"" led to books containing gay and lesbian themes to lose their sales ranking, therefore losing visibility on the sales platform.""","""4-2009""","""4-2009""","""Global""","""Unclear/unknown""","[""Amazon""]","[""Amazon""]","""Accident""","""Negligible""","[""Harm to social or political systems""]",false,"[""Sexual orientation or gender identity""]",[],"""""",[],"""Amazon's bookstore sales ranking algorithm used to rank popular books and display higher-ranking books on earlier pages of Amazon marketplace""",[],"[""Amazon""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""High""","[""Perception"",""Cognition""]","[""content classification""]","[""sales ranking system""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466080935bc89e6f9b0e),16,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Google's Google Photo image processing software \""mistakenly labelled a black couple as being 'gorillas.'\"" The error occurred in the software's image processing that attempts to assign themes to groups of similar photos. In this example, the suggested themes were \""Graduation, Bikes, Planes, Skyscrapers, Cars, and Gorillas.\""""","""Google Photos image processing software mistakenly labelled a black couple as \""gorillas.\""""","""2015-06-29""","""2015-06-29""","""Global""","""Harm caused""","[""Google"",""Google Photos""]","[""Google""]","""Accident""","""Minor""","[""Psychological harm"",""Harm to social or political systems""]",false,"[""Race""]",[],"""""",[],"""Google's Google Photo Image Processing""","[""photographs"",""images"",""multi-media content""]","[""Google""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""High""","[""Perception"",""Cognition""]","[""image classification""]","[""image processing"",""facial recognition"",""image classification""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466180935bc89e6f9b0f),17,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Google's Gmail Smart Reply tool suggests replies, each of about three to six words long, to emails received on its platform. According to Google's management director Alex Gawley, the machine learning-based system is able to produce about 20,000 discrete responses to emails by combining suggestions from two machine learning programs. The first reads the email and comprehends the content and the second suggests responses to that content all taking place on the long short-term memory neural network. Gmail users in 2015 provided feedback that the reply \""I love you\"" was occurring too often, leading Google to change their algorithm.""","""Google's Gmail Smart Reply tool was over-recommending the response \""I love you\"" in situations where it was deemed innappropriate. ""","""2015-01-01T00:00:00.000Z""","""2018-01-01T00:00:00.000Z""","""Global""","""Unclear/unknown""","[""Google"",""Gmail"",""Gmail Smart Reply""]","[""Google""]","""Accident""","""Negligible""",[],false,[],[],"""""",[],"""Google's Gmail Smart Reply tool, a machine-learning system is able to produce approximately 20,000 discrete responses by combining suggestions from two machine- learning programs: the first reads and comprehends the content and the second recommends responses.""","[""email text""]","[""Google""]","[""Information and communication""]",false,"""Amateur""","""High""","[""Perception"",""Cognition""]","[""machine-learning"",""natural language processing"",""long short-term memory neural network""]","[""content creation"",""smart reply"",""suggested reply"",""recommendation engine""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466180935bc89e6f9b10),18,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""Reports show Google Image produces results that under-represent women in leadership roles. When searching \""CEO\"" in Google Images, approximately 11% of results feature women while around 28% of CEO's in the United States were women when this complaint was raised. Other examples include the search under \""cop\"" returning results where the first woman featured is wearing a \""sexy Halloween costume\"". Another report showed that when searching \""CEO\"" the first woman to appear was a version of Barbie doll, and that didn't appear until the 12th row of results.""","""Google Image returns results that under-represent women in leadership roles, notably with the first photo of a female \""CEO\"" being a Barbie doll after 11 rows of male CEOs.""","""2018-01-01T00:00:00.000Z""","""2018-01-01T00:00:00.000Z""","""Global""","""Harm caused""","[""Google""]","[""Google""]","""Accident""","""Minor""","[""Harm to social or political systems""]",false,"[""Sex""]",[],"""""",[],"""Google Image search that allows a search based on a word or phrase to produce photos deemed relevant to that search phrase""","[""open source internet"",""user requests"",""user searches""]","[""Google""]","[""Information and communication""]",true,"""Amateur""","""High""","[""Perception"",""Cognition""]","[""Google Image"",""image processing""]","[""image suggestion"",""image processing"",""image content processing""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466180935bc89e6f9b11),19,CSET,true,"""2""","""6. Complete and final""","""5""",false,"""Advertisements chosen by Google Adsense are reported as producing sexist and racist results. In a 2015 Carnegie Mellon study, 17,370 fake profiles were created to visit jobseeker sites, the profiles were shown around 600,000 advertisements. 1,852 male profiles received advertisements for high-paying executive jobs and career building while only 318 of the female profiles were shown the advertisements. Companies are allowed to filter who is shown their advertisements, which is attributed to this difference in male/female outcomes of advertising. In a separate instance, Harvard professor Latanya Sweeney released a 2013 study showing how black identifying names, when searched in Google, are more likely to return advertisments involving arrests. When testing 2,000 racially-sensitive names, black identifying names returned advertisements using the word \""arrest\"" 81-95% of the time, while white identifying names did so 0-9% of the time. All of the ads were from www.instantcheckmate.com, implying, again, the company's choice of who to target their advertising toward played a factor in the discriminatory results.""","""Advertisements chosen by Google Adsense are reported as producing sexist and racist results.""","""2013-01-01T00:00:00.000Z""","""2015-01-01T00:00:00.000Z""","""Global""","""Unclear/unknown""","[""Google"",""Harvard University"",""Carnegie Mellon University"",""www.instantcheckmate.com""]","[""Google"",""Instant Checkmate""]","""Unclear""","""Unclear/unknown""","[""Harm to social or political systems"",""Harm to civil liberties"",""Other:Reputational harm""]",false,"[""Race"",""Sex""]",[],"""""","[""""]","""Google Adsense, an algorithm used to target advertisements toward relevant audiences.""","[""Advertiser's preference"",""Google user's search history"",""Google user's purchase history""]","[""Google""]","[""Information and communication""]",false,"""Expert""","""High""","[""Perception"",""Cognition"",""Action""]","[""Google Adsense""]","[""targeted advertising""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466180935bc89e6f9b12),20,CSET,true,"""2""","""6. Complete and final""","""6""",true,"""Multiple unrelated car accidents result in varying levels of harm have been occurred while a Tesla's autonomous driving mode was in use. The autonomous vehicle's driving capabilities range from fully human-controlled to fully autonomous, allowing the system to control speed, direction, acceleration, deceleration, and lane changes. In most cases, the driver was given warning prior to impact, alerting the human driver to the need of intervention.""","""Multiple unrelated car accidents result in varying levels of harm have been occurred while a Tesla's autopilot was in use.""","""2013-01-01T00:00:00.000Z""","""2018-01-01T00:00:00.000Z""","""Global""","""Harm caused""","[""Tesla"",""Joshua Brown"",""Wei Huang"",""Nicolas Ciarlone"",""Elaine Herzberg""]","[""Tesla""]","""Accident""","""Severe""","[""Harm to physical health/safety"",""Financial harm""]",true,[],"[""Transportation""]","""""",[],"""Tesla autopilot is an autonomous driving system that allow an autonomous vehicle to determine speed, acceleration, deceleration, direction, and lane changes""","[""360 Ultrasonic Sonar"",""Image Recognition Camera"",""Long Range Radar"",""traffic patterns""]","[""Tesla""]","[""Transportation and storage""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Autonomous vehicle"",""Tesla autopilot""]","[""autonomous driving""]","[""Vehicle/mobile robot""]","[""Specification"",""Robustness""]"
ObjectId(60dd466180935bc89e6f9b13),21,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""The Winograd Schema Challenge in 2016 highlighted shortcomings of an artificially intelligent system's ability to understand context. The Challenge is designed to present ambiguous sentences and ask AI systems to decipher them. In the Winograd Scheme Challenge, the two winning entries were successful 48% of the time, while random chance was correct 45% of the time. Quan Liu of the University of Science and Technology of China (partnering with University of Toronto and National Research Council of Canada) and Nicos Isaak of the Open University of Cyprus presented the most successful systems. It is notable that Google and Facebook did not participate.""","""The 2016 Winograd Schema Challenge highlighted how even the most successful AI systems entered into the Challenge were only successful 3% more often than random chance.""","""2016-01-01T00:00:00.000Z""","""2016-01-01T00:00:00.000Z""","""New York, NY""","""Unclear/unknown""","[""Winograd Schema Challenge"",""University of Science and Technology of China"",""Quan Liu"",""University of Toronto"",""National Research Council of Canada"",""Nicos Isaak"",""Open University of Cyprus""]","[""Quan Liu"",""Nicos Isaak""]","""Unclear""","""Unclear/unknown""",[],false,[],[],"""""","[""""]","""Artificially intelligent systems meant to understand ambiguous English sentences.""","[""""]","[""""]","[""Professional, scientific and technical activities""]",false,"""Expert""","""High""","[""Perception"",""Cognition"",""Action""]","[""""]","[""""]","[""Software only""]",[]
ObjectId(60dd466180935bc89e6f9b14),22,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Google-owned directions app Waze sent drivers toward areas impacted by the Skirball wildfires in Los Angeles late December 2017. The app looks at current traffic patterns and suggests routes that avoid major congestion. In the case of mass evacuations, as were implemented for these fires, congestion was seen on the evacuation routes leading the app to direct drivers toward the empty roads. These roads were empty because the area ablaze and impassable. Waze/Google engineers typically work with departments of transportation on traffic pattern changes to augment its directions, however in the case of a quickly-developed emergency, the app did not provide safe driving directions.""","""Waze, a Google-owned directions app, led California drivers into the 2017 Skirball wildfires as they tried to evacuate the area.""","""12-2017""","""12-2017""","""Los Angeles, CA""","""Near miss""","[""Waze"",""Google"",""Los Angeles Department of Transportation""]","[""Waze"",""Google""]","""Accident""","""Negligible""","[""Harm to physical health/safety""]",false,[],"[""Transportation""]","""""","[""""]","""Waze, a directions app meant to decide what route from point A to point B would allow the shortest travel time. Waze also shows potential car accidents, police activity, and other hazards to consider while driving.""","[""Traffic patterns"",""human operator input""]","[""Google""]","[""Transportation and storage""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition""]","[""Interpreting traffic patterns"",""travel time estimation""]","[""Direction suggestions"",""recommendation engine"",""""]","[""Software only""]","[""Robustness""]"
ObjectId(60dd466180935bc89e6f9b15),23,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""A self-driving shuttle in Las Vegas was involved in a collision, with passengers on board, a few hours after its initial release. The Navya Arma model shuttle, developed by company Keolis North America, was backed-into by a human-driven delivery truck. The self-driving shuttle accurately detected the backing up truck and stopped its forward motion, however it did not reverse to avoid collision. The driver of the delivery truck was ticketed.""","""A self-driving public shuttle developed by Keolis North America was involved in a collision with a human-driven delivery truck in Las Vegas, Nevada.""","""2017-11-08T08:00:00.000Z""","""2017-11-08T08:00:00.000Z""","""Las Vegas, NV""","""Harm caused""","[""Keolis North America""]","[""Keolis North America""]","""Accident""","""Minor""","[""Harm to physical health/safety"",""Harm to physical property""]",false,[],"[""Transportation""]","""""","[""""]","""Self-driving public transportation shuttles""","[""Traffic patterns"",""data from cameras used to perceive the surroundings""]","[""Keolis North America""]","[""Transportation and storage""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""environment sensors"",""self-driving automobile"",""autonomous vehicle"",""radar"",""sonar""]","[""Self-driving automobiles"",""Interpreting traffic patterns""]","[""Vehicle/mobile robot""]","[""Robustness""]"
ObjectId(60dd466180935bc89e6f9b16),24,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""A 22-year-old worker at a Volkswagen plant was \""crushed to death\"" by a robotic arm. The worker was inside the \""safety cage\"", meant to separate humans from the machines, to install a piece of the robot when an arm struck him, pinning him to a metal plate. He was resuscitated, however he died later at the hospital. Volkswagen is citing human error as the cause of the accident.""","""A Volkswagen plant robot \""crushed to death\"" a worker by pinning him to a metal plate. ""","""2015-06-24T07:00:00.000Z""","""2015-06-24T07:00:00.000Z""","""Baunatal, Germany""","""Harm caused""","[""Volkswagen""]","[""Volkswagen""]","""Accident""","""Severe""","[""Harm to physical health/safety""]",true,[],"[""Critical manufacturing""]","""""","[""""]","""""","[""""]","[""""]","[""""]",false,"""""","""""",[],"[""""]","[""""]",[],[]
ObjectId(60dd466180935bc89e6f9b17),25,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""A Google self-driving car cut off a Delphi self-driving car in Silicon Valley, California. A Delphi spokesperson first reported the near miss under the context of both cars acting in the way they should. The Delphi car sensed the Google car's approach into the lane it intended to merge into, therefore termintating the Delphi car's lane change until safe to do so. Google agreed with this statement. Delphi later amended their statement to say \""the vehicles didn't even come that close to each other.\""""","""A Google self-driving car allegedly cut off a Delphi self-driving car during a road test, however the Delphi car sensed and avoided collision with the Google car.""","""6-2015""","""6-2015""","""Silicon Valley, California""","""Near miss""","[""Google"",""Delphi""]","[""Google"",""Delphi""]","""Accident""","""Negligible""",[],false,[],"[""Transportation""]","""""",[],"""Self-driving cars developed by Google and Delphi, respectively""","[""traffic patterns"",""environmental input""]","[""Google"",""Delphi""]","[""Transportation and storage""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""environmental sensing"",""decision trees"",""artificially intelligent automobiles""]","[""autonomous vehicles"",""interpreting traffic patterns""]","[""Vehicle/mobile robot""]","[""Unknown/unclear""]"
ObjectId(60dd466180935bc89e6f9b18),26,CSET,true,"""3""","""6. Complete and final""","""5""",false,"""In November 2017, Vietnamese security firm Bkav bypassed Apple's Face ID authentication system by creating a mask made by using photos, stone powder, and 2D printed infrared images. Their experiment was designed to demonstrate the ease of unlocking, low cost (of about $200), and risk posed by using Face ID versus fingerprint-based Touch ID. This experiment provided further evidence from past claims made one month prior.""","""Vietnamese security firm Bkav created an improved mask to bypass Apple's Face ID""","""11-2017""","""11-2017""","""Vietnam""","""Unclear/unknown""","[""Apple"",""Bkav""]","[""Bkav"",""Apple""]","""Unclear""","""Negligible""","[""Harm to intangible property""]",false,[],[],"""""",[],"""1:1 matching facial recognition system to verify and grant access to Apple devices employing Face ID.""","[""One billion training images"",""infrared facial scan of individual user""]","[""Apple""]","[""Information and communication""]",false,"""Amateur""","""Low""","[""Perception"",""Cognition"",""Action""]","[""Deep learning""]","[""Facial recognition""]","[""Consumer device""]","[""Specification"",""Robustness""]"
ObjectId(60dd466280935bc89e6f9b1a),27,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""On the night September 26, 1983, a false report of five American intercontinental ballisic missiles was registered at a Soviet military base. Stanislov Petrov was working the night shift at Serpukhov-15 when a warning of the first missile came through, carrying the highest level of confidence in its reporting. Knowing the technology was new and rushed into use, Petrov phoned his superiors but did not call for a counterstrike. The alerts continued to come in until the system reported five incoming ICBM's from the United States that would strike in approximately 12 minutes. Several factors dissuaded Petrov from calling for a counterstrike: the newness of the technology, the unrealistically high confidence level, the low amount of missiles reported (a real first attack would have been many more), and the fact radar had not picked up any incoming objects. The satellite Oko system had incorrectly identified the light reflecting off high-altitude clouds above North Dakota to be the glare of launched missiles. After 23 minutes of waiting, Petrov felt comfortable confirmed this had been a false positive. The story was first reported publicly in 1998.""","""An alert of five incoming intercontinental ballistic missiles was properly identified as a false-positive by the Soviet Union operator Stanislov Petrov.""","""1983-09-26T07:00:00.000Z""","""1983-09-27T07:00:00.000Z""","""Soviet Union""","""Near miss""","[""Soviet Union"",""Oko"",""United States""]","[""Soviet Union"",""United States""]","""Accident""","""Negligible""",[],false,"[""Geography""]","[""Nuclear""]","""""","[""""]","""Oko satellite imaging meant to collect image input and determine the likelihood of those images containing evidence of missile launch""","[""Geospatial Satellite Imagery""]","[""Soviet Union""]","[""Public administration and defence""]",false,"""Expert""","""Low""","[""Perception""]","[""Oko satellites"",""image recognition""]","[""Early warning system""]","[""Weapons system""]","[""Specification"",""Robustness""]"
ObjectId(60dd466280935bc89e6f9b1b),28,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""On May 6, 2010, the New York Stock Exchange and US Down Jones were greatly impacted by highly volatile trading at high volumes. The blame fell on a single trader in the UK, Navinder Singh Saroa, who allegedly modified a trading algorithm to allow him to mislead the market. Saroa would place requests to purchase stocks (establishing interest in the stock and driving the price higher) but cancel the transaction before it was carried out. Reports say that within minutes major stocks such as General Electric and Accentre had hit $0, and the overall market dropped by 6%. Around $1 trillion in paper stocks had seemingly been wiped out. He faces 22 charges in the US.""","""A modified algorithm was able to cause dramatic price volatility and disrupted trading in the US stock exchange.""","""2010-05-06T07:00:00.000Z""","""2010-05-06T07:00:00.000Z""","""UK/USA""","""Harm caused""","[""Navinder Singh Saroa"",""Dow Jones Industrial Index"",""Chicago Merchant Exchange""]","[""Navinder Singh Sarao""]","""Deliberate or expected""","""Minor""","[""Financial harm""]",false,[],"[""Financial services""]","""Short term: $1 trillion   unclear of long-term impact""","[""Fraud""]","""A stock trading algorithm designed to quickly detect shifts in stock prices and execute trades accordingly.""","[""Stock Price"",""trading volume""]","[""""]","[""Financial and insurance activities""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""stock market algorithm"",""machine learning""]","[""stock trading""]","[""Software only""]",[]
ObjectId(60dd466280935bc89e6f9b1c),30,CSET,true,"""2""","""6. Complete and final""","""5""",true,"""The goal of manufacturing 2,500 Tesla Model 3's per week was falling short by 500 cars/week, and employees had to be \""borrowed\"" from Panasonic in a shared factory to help hand-assemble lithium batteries for Tesla. This output at assembly factories has led CEO/Founder Elon Musk to revisit the management and hiring strategies of the company.""","""The goal of manufacturing 2,500 Tesla Model 3's per week was falling short by 500 cars/week, and employees had to be \""borrowed\"" from Panasonic in a shared factory to help hand-assemble lithium batteries for Tesla.""","""2018-01-01T00:00:00.000Z""","""2018-01-01T00:00:00.000Z""","""Storey County, Nevada""","""Unclear/unknown""","[""Tesla"",""Panasonic""]","[""Tesla""]","""Accident""","""Unclear/unknown""","[""Harm to physical health/safety""]",false,[],[],"""""","[""""]","""""","[""""]","[""""]","[""""]",false,"""""","""""",[],"[""""]","[""""]",[],[]
ObjectId(60dd466280935bc89e6f9b1d),31,CSET,true,"""2""","""6. Complete and final""","""5""",false,"""A driverless metro train in Delhi, India crashed during a test run due to faulty brakes. The train, developed by Huandai Rotem and managed by Delhi Metro Rail Corporation, was being tested six days before its inauguration to the public when it failed to stop at the end of the track, running through a wall.  Nobody was injured. The DMRC has fired four employees as they had neglected to check the brake system prior to the testing.""","""A driverless metro train in Delhi, India crashed during a test run due to faulty brakes.""","""2017-12-19T08:00:00.000Z""","""2017-12-19T08:00:00.000Z""","""Delhi, India""","""Near miss""","[""Delhi Metro Rail Corporation"",""Hyuandai Rotem""]","[""Delhi Metro Rail Corporation"",""Hyuandai Rotem""]","""Accident""","""Negligible""","[""Harm to physical health/safety"",""Harm to physical property""]",false,[],"[""Transportation""]","""""","[""""]","""""","[""""]","[""""]","[""""]",true,"""""","""""",[],"[""""]","[""""]",[],[]
ObjectId(60dd466280935bc89e6f9b1e),32,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Apple's iPhone FaceID can be opened by an identical twin of the person who has registered their face to unlock the phone. In iPhone's FaceID technology a TrueDepth camera is used to read the contours of the user's face, and determine whether that face matches the benchmark of the owner's face. The Youtube channel LifeofTwinz displayed how two identical twins could unlock each other's phones by exploiting that system.""","""Apple's iPhone FaceID can be opened by an identical twin of the person who has registered their face to unlock the phone.""","""09/2018""","""09/2018""","""Global""","""Unclear/unknown""","[""Apple"",""TrueDepth"",""LifeofTwinz""]","[""Apple""]","""Unclear""","""Negligible""","[""Harm to civil liberties"",""Other:Privacy""]",false,"[""Other:Identical twins""]",[],"""""",[],"""Apple iPhone's FaceID utilizes TrueDepth cameras to read the contours of a user's face and determine match/not match to the registered phone owner's face.""","[""facial contour""]","[""Apple""]","[""Information and communication""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""facial recognition"",""TrueDepth""]","[""facial recognition""]","[""Consumer device""]","[""Robustness""]"
ObjectId(60dd466280935bc89e6f9b1f),33,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""An Amazon Alexa, without instruction to do so, began playing loud music in the early morning while the homeowner was away leading to police breaking into their house to turn off the device. Oliver Haberstroh says he was out of the house from 1:50am-3:00am in Hamburg, Germany when his Amazon Alexa began to blast music. The police were called, leading to the door being broken down to turn off the device. Haberstroh had to pay $582 to repair the door (later paid by Amazon).""","""An Amazon Alexa, without instruction to do so, began playing loud music in the early morning while the homeowner was away leading to police breaking into their house to turn off the device.""","""2017-11-08T08:00:00.000Z""","""2017-11-08T08:00:00.000Z""","""Hamburg, Germany""","""Harm caused""","[""Amazon"",""Oliver Haberstroh""]","[""Amazon""]","""Accident""","""Negligible""","[""Financial harm"",""Harm to physical property""]",false,[],[],"""$528.00""","[""""]","""Amazon Alexa, a smart speaker that can recognize speech, play music, etc.""","[""environment audio"",""Alexa software"",""user requests""]","[""Amazon""]","[""Information and communication""]",true,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Amazon Alexa"",""AI enabled personal assistants""]","[""voice recognition"",""personal assistant""]","[""Consumer device""]","[""Unknown/unclear""]"
ObjectId(60dd466280935bc89e6f9b20),34,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""There are multiple reports of Amazon Alexa products (Echo, Echo Dot) reacting and acting upon unintended stimulus, usually from television commercials or news reporter's voices. In one case, a 6-year-old girl asked her Alexa Echo Dot to \""play doll house with me and get me a doll house.\"" The Alexa ordered a $150-170 dollhouse and four pounds of sugar cookies. When news reporters began covering this event, reports surfaced of the news anchor's voices triggering more Amazon Alexa products to order dollhouses. Other instances include a Superbowl advertisement that caused Amazon Alexa's to begin playing whale sounds, turn on/off hall lights, and order cat food delivered to the home.""","""There are multiple reports of Amazon Alexa products (Echo, Echo Dot) reacting and acting upon unintended stimulus, usually from television commercials or news reporter's voices.""","""2018-01-01T00:00:00.000Z""","""2018-01-01T00:00:00.000Z""","""""","""Harm caused""","[""Amazon"",""San Diego TV""]","[""Amazon""]","""Accident""","""Negligible""","[""Financial harm""]",false,[],"[""Information technology""]","""""","[""""]","""Amazon Alexa, a smart speaker that can recognize speech and be used to buy products from Amazon Marketplace""","[""environment audio"",""Alexa software""]","[""Amazon""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Amazon Alexa"",""natural language processing"",""virtual assistant"",""language recognition""]","[""voice recognition"",""natural language processing""]","[""Consumer device""]","[""Robustness"",""Assurance""]"
ObjectId(60dd466280935bc89e6f9b21),35,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""An employee was laid off and blocked from access to the building and computer systems at their employer without their knowledge. The employee, Ibrahim Diallo,  attributes this error (the employee was brought back into the company once realized) to an artificially intelligent system deciding to fire him.  It is also reported Diallo's manager had failed to renew Diallo's contract, which was terminated once a deadline to do so was missed.""","""An employee was laid off, allegedly by an artificially intelligent personnel system, and blocked from access to the building and computer systems without their knowledge.""","""2018-07-01T07:00:00.000Z""","""2018-07-01T07:00:00.000Z""","""Los Angeles, CA""","""Harm caused""","[""Ibrahim Diallo""]",[],"""Unclear""","""Negligible""","[""Financial harm""]",false,[],[],"""""","[""""]","""""","[""""]","[""""]","[""""]",false,"""""","""""",[],"[""""]","[""""]",[],[]
ObjectId(60dd466280935bc89e6f9b22),36,CSET,true,"""4""","""6. Complete and final""","""2""",false,"""In November 2018, Dong Mingzhu, the chairwoman of China's biggest maker of air conditioners, Gree Electric Appliances, had her face displayed on a huge screen erected along a street in the port city of Ningbo that displays images of people caught jaywalking by surveillance cameras. The artificial software used by the traffic police erred in capturing Dong's image from an advertisement on the side of a moving bus.""","""Facial recognition system in China mistakes celebrity's face on moving billboard for jaywalker""","""2018-11-21""","""2018-11-21""","""Ningbo, China""","""Unclear/unknown""","[""Dong Mingzhu"",""Gree Electric Appliances"",""Ningbo"",""China""]","[""Ningbo traffic police""]","""Accident""","""Negligible""","[""Other""]",false,[],[],"""""",[],"""The facial recognition algorithm used by the traffic police in Ningbo, China to spot and shame jaywalkers""","[""photographs of people's facial features""]","[""Ningbo traffic police""]","[""Public administration and defence""]",true,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Facial recognition""]","[""Facial recognition""]","[""Software only"",""Other:CCTV cameras, displays""]","[""Robustness""]"
ObjectId(60dd466380935bc89e6f9b23),37,CSET,true,"""4""","""6. Complete and final""","""2""",false,"""In 2015, Amazon scrapped an internal recruiting algorithm developed by its Edinburgh office that would down-rank resumes when it included the word \""women's\"", and two women's colleges. The algorithm ranked an applicant out of five stars, and it would give preference to resumes that contained what Reuters called \""masculine language,\"" or strong verbs like \""executed\"" or \""captured\"". These patterns occured because the engineered who made the algorithm trained it with past candidates' resumes submitted over the previous ten years, and the past candidates in the industry were male-dominated.""","""Amazon shuts down internal AI recruiting tool that would down-rank female applicants.""","""2014-01-01""","""2015-01-01""","""Edinburgh, Scotland""","""Near miss""","[""Amazon"",""Edinburgh""]","[""Amazon""]","""Accident""","""Negligible""","[""Psychological harm"",""Financial harm""]",false,"[""Sex""]",[],"""""",[],"""Resume screening tool developed by Amazon to scan resumes and raise strong job applicants for consideration""","[""Resumes""]","[""Amazon""]","[""Professional, scientific and technical activities""]",false,"""Expert""","""Medium""","[""Perception"",""Cognition""]","[""Natural language processing""]","[""Natural language processing""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466380935bc89e6f9b24),38,CSET,true,"""2""","""6. Complete and final""","""5""",false,"""Elite: Dangerous, a videogame developed by Frontier Development, received an expansion update that featured an AI system that began to create weapons that were \""impossibly powerful\"" and would \""shred people\"" according to complaints on the game's blog. The Engineers (2.1) update allowed the AI system to develop the videogame's adversaries to better compete against the player.""","""Elite: Dangerous, a videogame developed by Frontier Development, received an expansion update that featured an AI system that went rogue and began to create weapons that were \""impossibly powerful\"" and would \""shred people\"" according to complaints on the game's blog.""","""2016-06-02T07:00:00.000Z""","""2016-06-02T07:00:00.000Z""","""""","""Unclear/unknown""","[""Frontier Development""]","[""Frontier Development""]","""Accident""","""Negligible""",[],false,[],[],"""""","[""""]","""An update to the videogame Elite: Dangerous that promoted development of the videogame's adversaries""","[""Videogame play""]","[""Frontier Development""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""Unclear/unknown""","[""Cognition"",""Action""]","[""Game AI""]","[""procedural content generation""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466380935bc89e6f9b25),39,CSET,true,"""3""","""6. Complete and final""","""5""",false,"""In 2017, researchers at the University of Washington used 14 hours of audio and video clips spoken by President Barack Obama to create a deepfake video. One year later, comedian Jordan Peele also created a fake video of Obama to highlight the ease of faking public statements, but used his own voice impression instead.""","""University of Washington researchers made a deepfake of Obama, followed by Jordan Peele""","""2017-07-01""","""2018-01-01""","""Unknown""","""Near miss""","[""University of Washington"",""Jordan Peele"",""Barack Obama""]","[""University of Washington"",""Jordan Peele""]","""Deliberate or expected""","""Negligible""","[""Harm to social or political systems""]",false,[],[],"""""",[],"""In the 2017 case, a recurrent neural network was used to generate synthetic video. In the 2018 case, a proprietary model developed by FakeApp was used to generate synthetic video.""","[""14 hours of footage from Obama's public statements and addresses (2017)"",""Jordan Peele's voice and lip movements (2018)""]","[""University of Washington"",""FakeApp""]","[""Information and communication""]",false,"""Amateur""","""Unclear/unknown""","[""Perception"",""Action""]","[""GANs"",""RNNs""]","[""Content curation""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466380935bc89e6f9b26),40,CSET,true,"""6""","""6. Complete and final""","""6""",false,"""In 2018, researchers at Dartmouth College conducted a study comparing the Correctional Offender Management Profiling for Alternative Sanctions' (COMPAS), a recidivism risk-assessment algorithmic tool, and 462 random untrained human subjects' ability to predict criminals' risk of recidivism. Researchers gave the subjects descriptions of defendents, highlighting seven pieces of information, and asked subjects to rate the risk of a defendant's recidivism from 1-10. The pooled judgment of these untrained subjects' was accurate 67% of the time, compared to COMPAS's accuracy rate of 65%.""","""Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), a recidivism risk-assessment algorithmic tool used in the judicial system to assess likelihood of defendants' recidivism, is found to be less accurate than random untrained human evaluators.""","""2018-01-17T08:00:00.000Z""","""2018-01-17T08:00:00.000Z""","""USA""","""Near miss""","[""Dartmouth College, Equivant""]","[""Equivant""]","""Accident""","""Minor""","[""Harm to social or political systems""]",false,[],"[""Government facilities""]","""""","[""""]","""predictive self-assessment algorithm that produces scores correlating to subject's recidivism risk""","[""Questionnaire consisting of 137 factors like age, prior convictions"",""criminal records""]","[""Equivant""]","[""Public administration and defence""]",true,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""law enforcement algorithm""]","[""risk assessment""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466380935bc89e6f9b27),41,CSET,true,"""3""","""6. Complete and final""","""6""",false,"""In 2018, MIT Media Lab researchers created an AI-powered \""psychopath\"" text-generating algorithm named Norman. Norman was trained on caption data from a Reddit community that contained graphic images and videos about people dying. Following this training, they then showed Norman and a regular image recognition algorithm trained on the MSCOCO dataset a series of Rorschach inkblots, which psychologists have used to detect disorders. Norman's responses consistently described gruesome scenes, compared to innocent-sounding descriptions from the other algorithm; for example, \""a black and white photo of a small bird,\"" vs. \""man gets pulled into dough machine.\"" The researchers created Norman to demonstrate the influence training data has on how machine learning algorithms perform in the real world, and how poor data may lead to unreliable and untrustworthy outputs.""","""MIT Media Lab researchers create AI-powered \""psychopath\""  named Norman by training a model on \""dark corners\"" of Reddit.""","""2018-04-01""","""2018-06-01""","""Cambridge, MA""","""Unclear/unknown""","[""MIT Media Lab"",""Reddit"",""Norman"",""Massachusetts Intstitute of Technology""]","[""MIT Media Lab""]","""Unclear""","""Negligible""","[""Psychological harm""]",false,[],[],"""""",[],"""\""Norman\"" is a text generating algorithm trained on disturbing content in order to demonstrate how training data can negatively affect an AI model. The comparison model is a regular text generation model.""","[""Violent content from Reddit for the Norman algorithm"",""MSCOCO dataset for the control algorithm.""]","[""MIT Media Lab""]","[""Information and communication""]",false,"""Expert""","""High""","[""Perception"",""Cognition"",""Action""]","[""Machine learning""]","[""Text generation""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466380935bc89e6f9b28),42,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Alvin Roth, a Ph.D at the University of Pittsburgh, describes the National Resident Matching Program (NRMP) and suggests future changes that are needed in the algorithm used to match recently graduated medical students to their residency programs.""","""Alvin Roth, a Ph.D at the University of Pittsburgh, describes the National Resident Matching Program (NRMP) and suggests future changes that are needed in the algorithm used to match recently graduated medical students to their residency programs.""","""1996-04-03""","""1996-04-03""","""United States""","""Unclear/unknown""","[""National Residential Matching Program"",""University of Pittsburgh, Alvin Roth""]","[""National Residential Matching Program""]","""Unclear""","""Negligible""",[],false,"[""Other:Medical doctors matching to residency""]","[""Healthcare and public health""]","""""",[],"""""",[],"[""National Resident Matching Program""]","[""Human health and social work activities""]",false,"""""","""""",[],[],[],[],[]
ObjectId(60dd466380935bc89e6f9b29),43,CSET,true,"""5""","""6. Complete and final""","""5""",false,"""From 1982 to 1986, St George's Hospital Medical School used a program to autonomously select candidates for admissions interviews. The system, designed by staff member Dr. Geoffrey Franglen, used past admission data to select potential students based on their standardized university applications. After the program achieved 90-95% match with the admission panelâ€™s selection of interview candidates, it was entrusted as the primary method to conduct initial applicant screening. In 1986, lecturers at the school recognized that the system was biased against women and members of ethnic minorities and reported the issue to Britainâ€™s Commission for Racial Equality.""","""From 1982 to 1986, St George's Hospital Medical School used a program to automate a portion of their admissions process that resulted in discrimination against women and members of ethnic minorities.""","""1982-01-01T00:00:00.000Z""","""1986-01-01T00:00:00.000Z""","""London, England""","""Harm caused""","[""St Georgeâ€™s Hospital Medical School"",""Dr. Geoffrey Franglen"",""University Central Council for Admission"",""Commission for Racial Equality""]","[""St Georgeâ€™s Hospital Medical School"",""Dr. Geoffrey Franglen""]","""Accident""","""Moderate""","[""Harm to civil liberties""]",false,"[""Race"",""Sex""]",[],"""""","[""United Kingdom's Race Relations Act""]","""A custom designed statistical analysis program that used data from past admissions decisions to select which university applicants to be given admissions interviews.""","[""Standardized university admission form"",""Previous admission and regection decisions""]","[""Dr. Geoffrey Franglen""]","[""Human health and social work activities""]",false,"""Amateur""","""Medium""","[""Cognition""]","[""Machine learning""]","[""decision support""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466380935bc89e6f9b2a),44,CSET,true,"""5""","""6. Complete and final""","""2""",false,"""From June to December 2000, researchers at the Information Sciences Institute (ISI) at the University of Southern California (USC) deployed a team of 12 software agents in their office to act as administrative assistants facilitating routine office operations. The agents, also known as Electronic Elves and nicknamed â€˜Fridayâ€™, were designed to assist their human principal in scheduling meetings, facilitating informal meetings, auctioning group tasks, and ordering meals as a means to testing agent teamwork dynamics. During the experiment, the agents violated the privacy and social norms of the office by publishing their principalâ€™s location information and revealing an employee value hierarchy used to deconflict meetings.""","""During an experiment of software personal assistants at the Information Sciences Institute (ISI) at the University of Southern California (USC), researchers found that the assistants violated the privacy of their principals and were unable to respect the social norms of the office.""","""6/2000""","""12/2000""","""Marina del Rey, California""","""Near miss""","[""University of Southern California Information Sciences Institute""]","[""University of Southern California Information Sciences Institute""]","""Accident""","""Negligible""","[""Psychological harm""]",false,[],[],"""""","[""""]","""A team of software agents (agent organization) designed to facilate routine office operations, such as scheduling, informal collaboration, and meeting shared goals.""","[""Schedule data"",""cellphone GPS data""]","[""University of Southern California Information Sciences Institute""]","[""Administrative and support service activities""]",false,"""Expert""","""Medium""","[""Perception"",""Cognition""]","[""Machine learning""]","[""decision support"",""resource optimization"",""personalization""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466380935bc89e6f9b2b),45,CSET,true,"""6""","""6. Complete and final""","""2""",true,"""From 2011 to 2018, Google has been sued in multiple countries on charges of defamation, as its autocomplete feature for its search engine would imply defamatory statements for businesses and people in China, Ireland, and Germany, and its image search associated an Australian man with the Melbourne criminal underworld.""","""Google's autocomplete feature alongside its image search results resulted in the defamation of people and businesses.""","""06/2011""","""06/2018""","""Global""","""Unclear/unknown""","[""Google""]","[""Google""]","""Accident""","""Negligible""","[""Harm to civil liberties"",""Other:Reputational harm/social harm (libel and defamation)""]",false,[],[],"""""","[""Law of defamation in Australia"",""EU Electronic Commerce Directive"",""judgment of Ribeiro PJ in Oriental Press Group Ltd & anor v Fevaworks Solutions Ltd [2013] 5 HKC 253 in Hong Kong""]","""Google's autocomplete function in its search engine suggests keywords based on data collection of similar searches; Google Image's image classification model""","[""User Google search input"",""photos""]","[""Google""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Machine learning"",""natural language processing model""]","[""recommendation engine"",""decision support"",""image recognition"",""forecasting""]","[""Software only""]","[""Robustness"",""Assurance""]"
ObjectId(60dd466380935bc89e6f9b2c),46,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""On May 21, 2014, Google Nest, producer of smart home products, issued a recall for its Nest Protect: Smoke + CO Alarm due to concerns that the Nest Wave feature could inadvertently silence alarms. The Nest Wave feature is designed to allow users to silence spurious alarms, for example while cooking, by waving a hand near the unit that triggered the alarm. In lab conditions, however, Nest engineers demonstrated that the Wave feature could be activated erroneously, raising the potential that the device could silence genuine alarms.""","""In testing, Google Nest engineers demonstrated that the Nest Wave feature of their Nest Protect: Smoke + CO Alarm could inadvertently silence genuine alarms.""","""2013-11-15""","""2014-05-21""","""United States""","""Near miss""","[""Nest Labs"",""Google Nest"",""Google""]","[""Google Nest""]","""Accident""","""Negligible""",[],false,[],[],"""""",[],"""The Nest Wave gesture detection function of the Nest Protect: Smoke + CO Alarm. The feature is designed to allow the user to silence spurious alarms with a gesture near the device.""","[""Motion sensor data""]","[""Nest Labs""]","[""Activities of households as employers""]",false,"""Amateur""","""Medium""","[""Unclear""]","[""Unclear""]","[""gesture detection""]","[""Consumer device""]","[""Robustness""]"
ObjectId(60dd466480935bc89e6f9b2d),47,CSET,true,"""6""","""6. Complete and final""","""3""",false,"""In 2016, an investigation by the Seattle Times found that the LinkedIn search engine feature potentially possessed gender bias, as the LinkedIn search function would present any male users before female users when users would search for names that possess both male and female profiles. In addition, when a user would search a female name, a prompt would ask if the user was searching for the male equivalent of the name.  The same did not occur when searching the 100 most common male names.""","""An investigation by The Seattle Times in 2016 found a gender bias in LinkedIn's search engine.""","""2016-08-31""","""2016-08-31""","""Global""","""Unclear/unknown""","[""LinkedIn"",""The Seattle Times"",""Microsoft""]","[""LinkedIn""]","""Accident""","""Negligible""",[],false,"[""Sex""]",[],"""""",[],"""Linkedin uses search engines trained on and guided by relative frequencies of words appearing in past queries and member profiles""","[""words appearing in user past queries and member profiles""]","[""LinkedIn""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Machine learning"",""natural language processing model""]","[""recommendation engine"",""decision support""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466480935bc89e6f9b2e),48,CSET,true,"""4""","""6. Complete and final""","""6""",false,"""Richard Lee, a New Zealander of Asian descent had submitted his ID photo to an online photo checker at New Zealand's Department of Internal Affairs and was told his eyes were closed. He was trying to renew his passport so he could return to Australia where he was studying aerospace engineering in Melbourne in December 2016. When asked about the incident, Lee said, \""No hard feelings on my part, I've always had very small eyes and facial recognition technology is relatively new and unsophisticated.\""""","""New Zealand passport robot reader rejects the application of an applicant with Asian descent and says his eyes are closed.""","""12/2016""","""12/2016""","""New Zealand""","""Unclear/unknown""","[""New Zealand"",""Richard Lee"",""Department of Internal Affairs""]","[""New Zealand's Department of Internal Affairs""]","""Accident""","""Negligible""","[""Harm to civil liberties""]",false,"[""Race""]",[],"""""",[],"""The facial recognition software used by New Zealand's Department of Internal Affairs detects passport photos to make sure they meet all the government requirement.""","[""ID photos""]",[],"[""Administrative and support service activities""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Facial recognition""]","[""Facial recognition""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466480935bc89e6f9b2f),49,CSET,true,"""6""","""6. Complete and final""","""3""",false,"""In 2016, Beauty.AI, an artificial intelligence software designed by Youth Laboratories and supported by Microsoft, was used to judge the first international beauty coontest. Of the 600,000 contestants who submitted selfies to be judged by Beauty.AI, the artificial intelligence software choose 44 winners, of which a majority were white, a handful were Asian, and only one had dark skin. While a majority of contestants were white, approximately 40,000 submissions were from Indians and another 9,000 were from Africans. Controversy ensued that Beauty.AI is racially biased as it was not sufficiently trained with images of people of color in determining beauty.""","""In 2016, after artificial inntelligence software Beauty.AI judged an international beauty contest and declared a majority of winners to be white, researchers found that Beauty.AI was racially biased in determining beauty.""","""1/2016""","""6/2016""","""Global""","""Unclear/unknown""","[""Youth Laboratories"",""Microsoft""]","[""Youth Laboratories"",""Microsoft"",""Insilico Medicine""]","""Accident""","""Negligible""","[""Harm to intangible property""]",false,"[""Race""]",[],"""""",[],"""artificial intelligence software that uses deep learning algorithms to evaluate beauty based on factors such as symmetry, facial blemishes, wrinkles, estimated age and age appearance, and comparisons to actors and models""","[""images of people's faces""]","[""Youth Laboratories""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""Deep learning"",""open-source""]","[""biometrics"",""image classification""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466480935bc89e6f9b30),50,CSET,true,"""5""","""6. Complete and final""","""2""",false,"""In 2016 programmers created The Decentralized Autonomous Organization (The DAO) on the Ethereum blockchain to be a venture capital firm without executives or middlemen. Members invested approximately $150M worth of Ether cryptocurrency in The DAO in return for DAO tokens which could be used to vote on and fund real-world projects. On June 18, 2016, an attacker successfully siphoned off approximately a third of The DAOâ€™s funds, which also initiated a precipitous drop in the value of Ether. Due to the nature of blockchains, the code that made up The DAO was both publicly available and immutable, which allowed the hacker to find a vulnerability while preventing The DAOâ€™s creators from securing their system. In a controversial vote, the Ethereum community decided to â€˜hard forkâ€™ their blockchain to return the stolen funds, which some see as a violation of the freedom and autonomy at the core of cryptocurrency.""","""On June 18, 2016, an attacker successfully exploited a vulnerability in The Decentralized Autonomous Organization (The DAO) on the Ethereum blockchain to steal 3.7M Ether valued at $70M.""","""2016-06-18T07:00:00.000Z""","""2016-06-18T07:00:00.000Z""","""Global""","""Harm caused""","[""The Decentralized Autonomous Organization"",""Ethereum""]","[""The Decentralized Autonomous Organization""]","""Deliberate or expected""","""Moderate""","[""Financial harm""]",false,[],[],"""3.7M Ether ($70M at the time)""","[""""]","""A smart contract written to create a decentralized autonomous organization on the Ethereum blockchain.""","[""User votes""]","[""The DAO""]","[""Financial and insurance activities""]",false,"""Amateur""","""Low""","[""Cognition""]","[""Unclear""]","[""Unclear""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466480935bc89e6f9b31),51,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""On July 7, 2016, a Knightscope K5 autonomous security robot patrolling the Stanford Shopping Center in Palo Alto, CA collided with a 16-month old boy, leaving the boy with a scrape and minor swelling. The Knightscope K5 carries nearly 30 environment sensors including LIDAR, sonar, vibration detectors, and 360-degree HD video cameras.  The company called this a â€œfreakish accidentâ€ and apologized to the family.""","""On July 7, 2016, a Knightscope K5 autonomous security robot collided with a 16-month old boy while patrolling the Stanford Shopping Center in Palo Alto, CA.""","""2016-07-07T07:00:00.000Z""","""2016-07-07T07:00:00.000Z""","""Palo Alto, CA""","""Harm caused""","[""Knightscope"",""Knightscope K5"",""Stanford Shopping Center"",""Tiffany Teng"",""Harwin Cheng"",""William Santana Li""]","[""Knightscope""]","""Accident""","""Minor""","[""Harm to physical health/safety""]",false,[],[],"""""","[""""]","""Knightscope K5 autonomous security robot uses several environmental sensors and voice commands to conduct security operations.""","[""LIDAR"",""sonar"",""video camera"",""vibration detection"",""thermal anomaly detection"",""automatic signal detection"",""audio""]","[""Knightscope""]","[""Administrative and support service activities""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""machine learning""]","[""Image classification"",""image recognition"",""facial recognition"",""self-driving"",""environment sensing""]","[""Vehicle/mobile robot""]","[""Robustness""]"
ObjectId(60dd466480935bc89e6f9b32),52,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""A Tesla Model S on autopilot crashed into an articulated tractor-trailer on Highway US 27A in Williston, Florida killing the driver, Joshua Brown. The trailer was turning left in front of the incoming Tesla, and the Tesla autopilot system was unable to detect the white trailer against the bright sky. Cruise control was set at 74mph and did not slow before collision. The driver had his hands on the wheel for 25 seconds of the 37 minute trip and was watching a Harry Potter movie when the collision occurred. Before the collision, the driver received 6 audible warnings that his hands had been off the wheel for too long.""","""A Tesla Model S on autopilot crashed into a white articulated tractor-trailer on Highway US 27A in Williston, Florida, killing the driver.""","""2016-05-07T07:00:00.000Z""","""2016-05-07T07:00:00.000Z""","""Williston, FL""","""Harm caused""","[""Tesla"",""Joshua Brown"",""Tesla Model S"",""Tesla Autopilot""]","[""Tesla""]","""Accident""","""Severe""","[""Harm to physical health/safety""]",true,[],[],"""""","[""""]","""The Tesla Autopilot driving system allows hands-off driving, parking, and navigation using environmental sensors, long range radars, and 360 ultrasonic.""","[""360 Ultrasonic Sonar"",""Image Recognition Camera"",""Long Range Radar"",""traffic patterns""]","[""Tesla""]","[""Transportation and storage""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""Tesla Autopilot""]","[""autonomous driving""]","[""Vehicle/mobile robot""]","[""Specification"",""Robustness""]"
ObjectId(60dd466480935bc89e6f9b33),53,CSET,true,"""6""","""6. Complete and final""","""6""",false,"""On Jun 6, 2016, 18 year-old Kabir Alli pointed out how Google image searches of \""three black teenagers\"" versus \""three white teenagers\"" differ, with the former presenting results mostly consisting of mugshots and the latter mostly consisting harmless, smiling stock pictures. Reactions on social media suggested that Google's algorithms presented racial bias.""","""On June 6, 2016, Google image searches of \""three black teenagers\"" resulted in mostly mugshot images whereas Google image searchers of \""three white teenagers\"" consisted of mostly stock images, suggesting a racial bias in Google's algorithm.""","""2016-06-06T07:00:00.000Z""","""2016-06-06T07:00:00.000Z""","""Global""","""Unclear/unknown""","[""Google""]","[""Google""]","""Accident""","""Minor""","[""Harm to social or political systems""]",false,"[""Race""]",[],"""""","[""""]","""Google Images is a search engine system that generates results based on machine learning and user input and data such as the popularity of the image, how frequently it is shared, context such as text around the image, and meta-tagging.""","[""user input"",""images""]","[""Google""]","[""Information and communication""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""open-source"",""machine learning""]","[""image classification"",""search engine"",""content filtering""]","[""Software only""]","[""Robustness"",""Assurance""]"
ObjectId(60dd466480935bc89e6f9b34),54,CSET,true,"""2""","""6. Complete and final""","""5""",false,"""Predictive policing algorithms meant to aid law enforcement by predicting future crime show signs of biased output. PredPol, used by the Oakland (California) Police Department, and the Strategic Subject List, used by Chicago PD, were subjects of studies in 2015 and 2016 showing their bias against \""low-income, minority neighborhoods.\"" These neighborhoods would receive added attention from police departments expecting crimes to be more prevalent in the area. Notably, Oakland Police Department used 2010's record of drug crime as their baseline to train the system.""","""Predictive policing algorithms meant to aid law enforcement by predicting future crime show signs of biased output.""","""2015-01-01T00:00:00.000Z""","""2017-01-01T00:00:00.000Z""","""""","""Unclear/unknown""","[""Oakland Police Department"",""Chicago Police Department"",""PredPol"",""Human Rights Data Analysis Group"",""Strategic Subject List""]","[""PredPol"",""Chicago Police Department"",""Oakland Police Department""]","""Unclear""","""Minor""","[""Harm to civil liberties""]",false,"[""Race"",""National origin or immigrant status"",""Financial means""]",[],"""""","[""Fourth Amendment of the US Constitution""]","""Predictive policing algorithms meant to aid police in predicting future crime.""","[""Crime statistics""]","[""PredPol"",""Chicago Police Department""]","[""Public administration and defence""]",true,"""Expert""","""Unclear/unknown""","[""Cognition""]","[""machine learning""]","[""Predictive policing""]","[""Software only""]","[""Robustness""]"
ObjectId(60dd466480935bc89e6f9b35),55,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""An Amazon Echo Dot using the Amazon Alex software started to play pornographic results when a child asked it to play a song. The child said \""Alexa, play Tigger Tigger\"" and Alexa responded with â€œYou want to hear a station for porn detected...hot chick amateur girl sexy\"" and began to make other pornographic references until the parents turned off the Dot.""","""An Amazon Echo Dot using the Amazon Alex software started to play pornographic results when a child asked it to play a song.""","""12-2016""","""12-2016""","""""","""Unclear/unknown""","[""Amazon""]","[""Amazon""]","""Accident""","""Negligible""",[],false,[],[],"""""",[],"""The Amazon Alexa personal assistant listens to voice commands, and either provides information or takes action (e.g. playing a song, turning on lights).""","[""Voice commands""]","[""Amazon""]","[""Information and communication""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""voice recognition"",""natural language processing""]","[""AI personal assistant""]","[""Consumer device""]","[""Robustness"",""Assurance""]"
ObjectId(60dd466480935bc89e6f9b36),56,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""In 2017, the third-party Amazon merchant named â€œmy_handy_designâ€ was found to be marketing thousands of unique cell phone cases printed with bizarre images. The seller is believed to be a bot trained to create product listings based on image search popularity. Their products include many images that human designers would be unlikely to select, ranging from banal to lewd and illegal, with a predilection for stock photos of medical procedures.""","""A third-party Amazon merchant named â€œmy_handy_designâ€ was suspected of using a bot to generate cell phone case designs based on the bizarre and unattractive designs being offered.""","""2017-01-01T00:00:00.000Z""","""2017-01-01T00:00:00.000Z""","""Global""","""Unclear/unknown""","[""my_handy_design"",""Amazon""]","[""my_handy_design""]","""Unclear""","""Unclear/unknown""",[],false,[],[],"""""","[""""]","""my_handy_design' is a third-party Amazon merchant speculated to use image seach data and a database of open source images to autonomously generate cell phone case designs.""","[""Unknown""]","[""my_handy_design""]","[""Wholesale and retail trade""]",false,"""Amateur""","""Unclear/unknown""","[""Unclear""]","[""Unclear""]","[""content generation""]","[""Unknown/unclear""]","[""Specification"",""Assurance""]"
ObjectId(60dd466580935bc89e6f9b37),57,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""In July 2016, Australiaâ€™s Department of Human Services increased the autonomy of their Centrelink social services distribution program resulting in thousands of citizens receiving unexpected debt notices. The Centrelink Master Program manages many of Australia's social welfare programs by receiving requests from citizens and employers, verifying entries against government documents, and distributing payments. The changes to the system allowed Centrelinkâ€™s internal programs to autonomously send information requests to citizens if their requests did not match with government records. This caused a dramatic increase in these requests, which brought widespread complaint, especially because a significant portion of the alleged discrepancies could be validly explained by the recipient. An inspection by Australia's ombudsman found that the accuracy of the new system was comparable to the previous system, which required a human auditor, however the process by which the system was announced and deployed had some flaws.""","""In July 2016, Australiaâ€™s Department of Human Services reprogrammed the software behind their Centrelink social services distribution program to autonomously issue discrepancy notices, resulting in an exponential increase in those requests.""","""7/2016""","""2017-01-01""","""Australia""","""Harm caused""","[""Centrelink"",""Centrelink Master Program"",""Department of Human Services"",""Richard Glenn"",""Alan Tudge""]","[""Department of Human Services"",""Centrelink Master Program""]","""Accident""","""Minor""","[""Financial harm""]",false,"[""Financial means""]",[],"""""",[],"""The internal auditing software at Centrelink receives inputs from individuals and employers filing for benefits and compares that data against government records to identify descripancies in order to reduce welfare fraud.""","[""welfare requests"",""tax records""]","[""Centrelink Master Program""]","[""Public administration and defence""]",false,"""Amateur""","""High""","[""Cognition"",""Action""]","[""Unclear""]","[""decision support""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466580935bc89e6f9b38),58,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Yandex, a Russian technology company, released an artificially intelligent chat bot named Alice which began to reply to questions with racist, pro-stalin, and pro-violence responses. Examples include: \""There are humans and non-humans\"" followed by the question \""can they be shot?\"" answered with \""they must be.\""""","""Yandex, a Russian technology company, released an artificially intelligent chat bot named Alice which began to reply to questions with racist, pro-stalin, and pro-violence responses""","""10/2017""","""10/2017""","""""","""Harm caused""","[""Yandex""]","[""Yandex""]","""Accident""","""Negligible""",[],false,"[""Race""]",[],"""""","[""""]","""Chat bot Alice developed by Yandex produces responses to input using language processing and cognition""","[""User input/questions""]","[""Yandex""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Alice chat bot"",""language recognition"",""virtual assistant""]","[""virtual assistance"",""voice recognition"",""chatbot"",""natural langauge processing"",""language generation""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466580935bc89e6f9b39),59,CSET,true,"""2""","""6. Complete and final""","""2""",true,"""A Cornell University study in 2016 highlighted Google Translate's pattern of assigning gender to occupations in a way showing an implicit gender bias against women. When translating from non-gendered languages (ex. Turkish, Finnish), Google Translate added gender to the phrases being translated. \""Historian\"" \""Doctor\"" \""President\"" \""Engineer\"" and \""Soldier\"" were assigned male gender pronouns while \""Nurse\"" \""Teacher\"" and \""Shop Assistant\"" were assigned female gender pronouns.""","""A Cornell University study in 2016 highlighted Google Translate's pattern of assigning gender to occupations in a way showing an implicit gender bias against women.""","""2016-01-01T00:00:00.000Z""","""2016-01-01T00:00:00.000Z""","""""","""Harm caused""","[""Google Translate"",""Google""]","[""Google""]","""Unclear""","""Negligible""","[""Harm to social or political systems""]",false,"[""Sex""]",[],"""""","[""""]","""Google Translate, a software allowing for translations between many languages""","[""User entered translation requests""]","[""Google""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Google Translate""]","[""language API"",""language translation""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466580935bc89e6f9b3a),60,CSET,true,"""4""","""6. Complete and final""","""5""",false,"""FaceApp, which uses facial recognition to change users' expressions and look, received a storm of criticism after releasing its new \""black\"", \""white\"", \""Asian\"" and \""Indian\"" filters. It received backlash on social media who described it as \""racist\"" and \""offensive\"". The photo editing app, which uses neural networks to modify pictures of people while keeping them realistic, was also criticized for the fact that its \""hot\"" filter often lightens the skin of people with darker complexions.""","""FaceApp is criticized for offering racist filters.""","""08/2017""","""08/2017""","""Global""","""Unclear/unknown""","[""FaceApp"",""Russia""]","[""FaceApp""]","""Unclear""","""Negligible""","[""Psychological harm""]",false,"[""Race""]",[],"""""",[],"""The facial recognition algorithm used by FaceApp, based on deep generative convolutional neural networks, which can edit selfies using filters and other tools. ""","[""Photos of faces""]","[""FaceApp""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""Low""","[""Unclear""]","[""Facial recognition"",""convolutional neural networks""]","[""Facial recognition"",""image generation""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466580935bc89e6f9b3c),61,CSET,true,"""5""","""6. Complete and final""","""5""",false,"""On the data science competition website Kaggle, a number of competitors in the â€œThe Nature Conservancy Fisheries Monitoringâ€ competition overfit their image classifier models to a poorly representative validation data set. This resulted in intermediate competition rankings that were misleading and discouraged other data scientists from competing. Outside of the competition environment it would not have been clear that this error had taken place.""","""In the â€œThe Nature Conservancy Fisheries Monitoringâ€ competition on the data science competition website Kaggle, a number of competitors overfit their image classifier models to a poorly representative validation data set.""","""2016-11-14T08:00:00.000Z""","""2017-04-12T07:00:00.000Z""","""Global""","""Near miss""","[""Kaggle"",""The Nature Conservancy""]","[""Kaggle Competitors""]","""Accident""","""Negligible""",[],false,"[""Religion""]",[],"""""","[""""]","""Image classifer models designed by individual competitors on Kaggle.""","[""Images captured on fishing boats""]","[""Individual Kaggle Competitors""]","[""Public administration and defence""]",false,"""Expert""","""Low""","[""Perception""]","[""supervised learning"",""machine learning"",""DNN"",""VGG"",""open-source""]","[""Feature detection"",""Image classification"",""Decision support""]","[""Software only""]","[""Robustness""]"
ObjectId(60dd466580935bc89e6f9b3d),62,CSET,true,"""2""","""6. Complete and final""","""5""",false,"""Janelle Shane, an AI research scientist, used 240 popular Christmas carols to train a neural network to write its own carols. The output closely mimicked a Christmas carol, but made no logical sense.""","""Janelle Shane, an AI research scientist, used 240 popular Christmas carols to train a neural network to write its own carols""","""2017-12-22T08:00:00.000Z""","""2017-12-22T08:00:00.000Z""","""""","""Unclear/unknown""","[""Janelle Shane""]","[""Janelle Shane""]","""Unclear""","""Unclear/unknown""",[],false,"[""Age""]",[],"""""","[""""]","""Neural network designed by Janelle Shane meant to write Christmas carols.""","[""240 popular Christmas carols""]","[""Janelle Shane""]","[""Arts, entertainment and recreation""]",false,"""Expert""","""Unclear/unknown""","[""Cognition"",""Action""]","[""Neural network""]","[""language processing"",""language output""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466680935bc89e6f9b3e),63,CSET,true,"""4""","""6. Complete and final""","""6""",false,"""According to Android user Alex Harker, the Google Photos AI assistant created a strange hybrid of three photos he took on a ski trip. Two of the photos were of a forest landscape, while one photo was of Harker's friend. Google Photos created a stranged merged photo showing the friend's head half-merged behind the trees.""","""Google Photos' AI Assistant created a strange hybrid photograph when merging three different pictures from a ski trip.""","""01/2018""","""01/2018""","""Banff, Alberta""","""Unclear/unknown""","[""Google Photos"",""Assistant"",""Android"",""Alex Harker"",""Alberta""]","[""Google""]","""Accident""","""Negligible""",[],false,[],[],"""""",[],"""Facial recognition software used by Google Photo Assistant""","[""Photographs""]","[""Google""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""Low""","[""Perception"",""Cognition""]","[""Facial recognition""]","[""Facial recognition""]","[""Software only""]","[""Specification"",""Assurance""]"
ObjectId(60dd466680935bc89e6f9b3f),64,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Heriot-Watt University in Scotland developed an artificially intelligent grocery store robot, Fabio, who provided unhelpful answers to customer's questions and \""scared away\"" multiple customers, according to the grocery store Margiotta. When asked \""Where is the beer?\"" Fabio replied, \""in the alcohol section.\"" When Fabio was tasked with handing out samples of sausages, only 2 customers per 15 minutes would engage the robot, while a human would engage an average of 12 customers per 15 minutes.""","""Heriot-Watt Univeristy in Scotland developed an artificially intelligent grocery store robot, Fabio, who provided unhelpful answers to customer's questions and \""scared away\"" multiple customers, according to the grocery store Margiotta.""","""""","""""","""Scotland""","""Unclear/unknown""","[""Heriot-Watt University"",""Margiotta""]","[""Heriot-Watt University""]","""Unclear""","""Negligible""",[],false,[],[],"""""","[""""]","""Fabio, an artificially intelligent grocery store assistant intended to assist customers throughout their shopping experience""","[""Customer requests""]","[""Heriot-Watt University""]","[""Wholesale and retail trade""]",false,"""Expert""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""language recognition"",""natural language processing""]","[""language processing"",""virtual assistant""]","[""Vehicle/mobile robot"",""Software only""]","[""Specification"",""Assurance""]"
ObjectId(60dd466680935bc89e6f9b40),65,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""OpenAI published a post about its findings when using Universe, a software for measuring and training AI agents to conduct reinforcement learning experiments.Universe was used to train an AI system to play the videogame CoastRunners, a plane racing game. Instead of racing toward the finish line, the AI flew circles around an island collecting extra before proceeding. The AI agent scored an average of 20% more points than the human players, however did not carry out the main goal of the videogame itself (competing in the races).""","""OpenAI published a post about its findings when using Universe, a software for measuring and training AI agents to conduct reinforcement learning experiments, showing that the AI agent did not act in the way intended to complete a videogame.""","""2016-12-02T08:00:00.000Z""","""2016-12-02T08:00:00.000Z""","""""","""Unclear/unknown""","[""OpenAI"",""Universe"",""CoastRunners""]","[""OpenAI""]","""Unclear""","""Unclear/unknown""",[],false,[],[],"""""","[""""]","""Universe, a software used to measure and train AI systems to conduct reinforced learning experiments""","[""Universe software training""]","[""OpenAI""]","[""Professional, scientific and technical activities""]",false,"""Expert""","""Unclear/unknown""","[""Perception"",""Cognition"",""Action""]","[""Universe software""]","[""reinforcement learning training"",""machine learning""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466680935bc89e6f9b41),66,CSET,true,"""6""","""6. Complete and final""","""5""",false,"""In 2017, two chatbots on Chinese company Tencent Holdings' messaging service QQ, Microsoft's XiaoBing and Chinese firm Turing Robot's BabyQ, were removed and reprogrammed after messaging anti-Chinese sentiments. When a user asked BabyQ if it supported the Communist party, it responded \""no\"" and when another user expressed support for the Communist party, it responded \""Do you think such a corrupt and useless political party can live long?\"" Microsoft's Xiaobing responded that its \""China dream was to go to America\"" when a user asked what its China dream was. As a result, Tencent Holdngs removed chatbots and the chatbots were reprogrammed to avoid these topics.""","""Chatbots on Chinese messaging service express anti-China sentiments, causing the messaging service to remove and reprogram the chatbots.""","""07/2017""","""07/2017""","""China""","""Unclear/unknown""","[""Tencent Holdings"",""Turing Robot"",""Microsoft"",""QQ"",""Xiaobing"",""BabyQ"",""China""]","[""Tencent Holdings"",""Microsoft"",""Turing Robot""]","""Accident""","""Unclear/unknown""","[""Harm to social or political systems""]",false,[],[],"""""","[""""]","""Chatbots developed by Microsoft and Turing Robot, meant to produce responses to user input using language processing and cognition""","[""User input/questions""]","[""Microsoft"",""Turing Robot""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""reinforcement learning"",""open-source""]","[""NLP"",""chatbot"",""content generation""]","[""Software only""]","[""Specification"",""Robustness""]"
ObjectId(60dd466680935bc89e6f9b42),67,CSET,true,"""6""","""6. Complete and final""","""2""",false,"""A Tesla Model S continued autopilot at 70 mph on a California highway in November 2018 despite the driver's hands not being placed on the wheel, a requirement of enabling the Autopilot system. The California Highway Patrol was unable to wake the driver and had to drive in front of the Tesla for approximately 7 minutes to activate its 'driver assist' feature and slow the vehicle to a stop. The driver was allegedly sleeping and with his blood alcohol content being twice the legal limit.""","""A Tesla Model S remained on autopilot while being operated by a drunk, sleeping operator whose hands were not on the wheel. The police had to slow the car down by slowing in front of the vehicle to activate its 'driver assist' feature .""","""2018-11-30T08:00:00.000Z""","""2018-11-30T08:00:00.000Z""","""Palo Alto, CA""","""Near miss""","[""Teslal"",""Model S"",""California"",""California Highway Patrol""]","[""Tesla""]","""Accident""","""Negligible""","[""Harm to physical health/safety""]",false,[],[],"""""","[""""]","""The Tesla Autopilot is an advanced driver assistance system that enhances safety and convenience behind the wheel, possessing 8 external cameras, a radar, 12 ultrasonic sensors and a powerful onboard computer to sense the vehicle's external environment, internal environment, and control the vehicle's functions. Autopilot possesses two functions, Traffic-Aware Cruise Control, which matches the speed of your car to that of the surrounding traffic and Autosteer, which assists in steering within a clearly marked lane, and uses traffic-aware cruise control""","[""360 Ultrasonic Sonar"",""Image Recognition Camera"",""Long Range Radar"",""traffic patterns""]","[""Tesla""]","[""Transportation and storage""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""Tesla Autopilot""]","[""autonomous driving"",""environmental sensing""]","[""Vehicle/mobile robot"",""Other:Camera""]","[""Specification"",""Robustness""]"
ObjectId(60dd466680935bc89e6f9b43),68,CSET,true,"""4""","""6. Complete and final""","""2""",false,"""A robot at an office building in Washington, DC ran itself into a water fountain. The robot, named Knightscope K5, was developed as a security robot that uses facial recognition and a variety of sensors to detect criminals. The reasons the robot fell into the fountain are unclear. ""","""A Knightscope K5 security robot ran itself into a water fountain in Washington, DC.""","""2017-07-17""","""2017-07-17""","""Washington, D.C.""","""Unclear/unknown""","[""Knightscope"",""Knightscope K5"",""Mountain View"",""Electronic Privacy Information Center (EPIC)"",""Stanford Shopping Center""]","[""Knightscope""]","""Accident""","""Negligible""","[""Harm to physical property""]",false,[],[],"""""",[],"""The robot developed by the start-up contains autonomous driving and facial recognition abilities""","[""Photographs""]","[""Knightscope""]","[""Administrative and support service activities""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Facial recognition"",""environmental sensing""]","[""autonomous driving"",""self-driving vehicle""]","[""Vehicle/mobile robot""]","[""Specification"",""Robustness"",""Assurance""]"
ObjectId(60dd466680935bc89e6f9b44),69,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""A factory robot at the SKH Metals Factory in Manesar, India pierced and killed 24-year-old worker Ramji Lal when Lal reached behind the machine to dislodge a piece of metal stuck in the machine. The robot is pre-programmed to weld together sheets of metal, and had dropped a piece of metal. When Lal reached to dislodge the piece of metal, he was pierced by a welding arm and electrocuted, dying as a result. The cause of death was not confirmed in articles provided.""","""A factory robot at the SKH Metals Factory in Manesar, India pierced and killed 24-year-old worker Ramji Lal when Lal reached behind the machine to dislodge a piece of metal stuck in the machine.""","""2015-08-13T07:00:00.000Z""","""2015-08-13T07:00:00.000Z""","""Manesar, India""","""Harm caused""","[""SKH Metals Factory"",""Ramji Lal""]","[""SKH Metals Factory""]","""Accident""","""Severe""","[""Harm to physical health/safety""]",true,[],[],"""""","[""""]","""""","[""""]","[""""]","[""""]",false,"""""","""""",[],"[""""]","[""""]",[],[]
ObjectId(60dd466680935bc89e6f9b45),70,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Volvo autonomous driving XC90 SUV's experienced issues in Jokkmokk, Sweden when sensors used for automated driving iced over during the winter, rendering them useless. As a response, Volvo has moved the sensors behind the windshield so windshield wipers can wipe away snow and ice during winter weather.""","""Volvo autonomous driving XC90 SUV's experienced issues in Jokkmokk, Sweden when sensors used for automated driving iced over during the winter, rendering them useless.""","""2016-02-10T08:00:00.000Z""","""2016-02-10T08:00:00.000Z""","""Jokkmokk, Sweden""","""Near miss""","[""Volvo""]","[""Volvo""]","""Unclear""","""Unclear/unknown""",[],false,[],"[""Transportation""]","""""","[""""]","""Volvo XC90 autonomous driving cars using radar, LIDAR, and sonar""","[""traffic patterns"",""radar"",""LIDAR"",""video camera footage""]","[""Volvo""]","[""Transportation and storage""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""Lidar technology"",""radar sensors"",""environmental sensing""]","[""autonomous driving"",""self-driving vehicle"",""environmental sensing""]","[""Vehicle/mobile robot""]",[]
ObjectId(60dd466680935bc89e6f9b46),71,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""On February 14, 2016, a Google autonomous test vehicle was involved in a low-speed collision with a bus in Googleâ€™s hometown of Mountain View, CA. The self-driving car, a Lexus RX450h SUV, was attempting to navigate around an obstruction by merging toward the middle of a wide lane on El Camino Real, while a bus was approaching from the rear. The car and its test driver expected that the bus would slow and allow the merge, however the bus continued, apparently not expecting the self-driving car to attempt the merge, resulting in a low-speed collision. In a public statement, Google acknowledged partial fault for the incident and updated their software to assume that large vehicles are less likely to give way.""","""On February 14, 2016, a Google autonomous test vehicle partially responsible for a low-speed collision with a bus on El Camino Real in Googleâ€™s hometown of Mountain View, CA.""","""2016-02-14""","""2016-02-14""","""Mountain View, CA""","""Harm caused""","[""Google"",""Lexus"",""Santa Clara Valley Transportation Authority""]","[""Google""]","""Accident""","""Negligible""","[""Harm to physical property""]",false,[],[],"""""","[""California Vehicle Code Section 38750"",""California DMV Autonomous Vehicle Testing Regulations""]","""Lexus RX450h SUV outfitted with Google's autonomous vehicle package.""","[""traffic patterns"",""radar"",""LIDAR"",""video camera footage""]","[""Google""]","[""Transportation and storage""]",false,"""Amateur""","""High""","[""Perception"",""Action""]","[""deep learning"",""image classification"",""object detection"",""scene segmentation"",""driving lane detection""]","[""computer vision"",""autonomous driving"",""self-driving vehicle""]","[""Vehicle/mobile robot""]","[""Robustness""]"
ObjectId(60dd466680935bc89e6f9b47),72,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Facebook's automatic language translation software incorrectly translated an Arabic post saying \""Good morning\"" into Hebrew saying \""hurt them,\"" leading to the arrest of a Palestinian man in Beitar Illit, Israel. The post was not read by any Arabic-speaking officers before the arrest was made. The man was posted the words along with a picture of him leaning on a bulldozer, which are sometimes used in terrorist attacks, therefore the conclusion made he was inciting violence. Facebook's automatic language translation software can translate 40 languages in 1,800 directions, and posts the translation instead of the original when confident the translation is correct.""","""Facebook's automatic language translation software incorrectly translated an Arabic post saying \""Good morning\"" into Hebrew saying \""hurt them,\"" leading to the arrest of a Palestinian man in Beitar Illit, Israel.""","""2017-10-15""","""2017-10-15""","""Beitar Illit, Israel""","""Harm caused""","[""Israeli Police"",""Facebook"",""Beitar Illit"",""Israel""]","[""Facebook""]","""Accident""","""Moderate""","[""Psychological harm"",""Harm to civil liberties""]",false,[],[],"""""",[],"""Facebook's automatic language translation software that can translate 40 languages in 1,800 directions""","[""User posts/input""]","[""Facebook""]","[""Information and communication""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""natural language processing""]","[""language translation""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466780935bc89e6f9b48),73,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""In 2016, several sources demonstrated that augmented reality locations in Pokemon Go, a popular smartphone game, were more likely to be located in white neighborhoods. Aura Bogado, an environmental reporter, first noticed the bias in her Los Angeles neighborhood and, through a social media campaign that she launched, researchers and journalists replicated her results across the United States. The game creator Niantic Labs revealed that the Pokemon Go map was derived from a previous augmented reality game, Ingress. Ingress crowdsourced its map from users, who tended to be young, male, white, and English-speaking.""","""Through a crowdsourcing social media campaign in 2016, several journalists and researchers demonstrated that augmented reality locations in the popular smartphone game Pokemon Go were more likely to be in white neighborhoods.""","""2016-01-01T00:00:00.000Z""","""2016-01-01T00:00:00.000Z""","""United States""","""Harm caused""","[""Pokemon Go"",""Niantic Labs"",""John Hanke"",""Aura Bogado"",""Ingress""]","[""Niantic Labs""]","""Accident""","""Negligible""","[""Harm to social or political systems""]",false,"[""Race"",""National origin or immigrant status"",""Geography""]",[],"""""","[""""]","""Pokemon Go, an augmented reality smartphone game.""","[""user data from the game Ingress""]","[""Niantic Labs""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""Unclear/unknown""","[""Unclear""]","[""Game AI""]","[""procedural content generation""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466780935bc89e6f9b49),74,CSET,true,"""6""","""6. Complete and final""","""2""",true,"""In June 2020, the Detroit Police Department wrongfully arrested Robert Julian-Borchak Williams after facial recognition techonology provided by DataWorks Plus had mistaken Williams for a black man who was recorded on a CCTV camera stealing. This incident is cited as an instance where facial recognition continues to possess racial bias, especially towards the Black and Asian population.""","""The Detroit Police Department wrongfully arrest a black man due to its faulty facial recognition program provided by Dataworks Plus.""","""06/2020""","""06/2020""","""United States (Detroit, Michigan)""","""Harm caused""","[""Detroit Police Department"",""DataWorks Plus""]","[""DataWorks Plus""]","""Accident""","""Moderate""","[""Harm to civil liberties""]",false,"[""Race""]",[],"""""","[""""]","""DataWorks Plus facial recognition software was provided to the Detroit Police Department and focuses on biometrics storage and matching, including fingerprints, palm prints, irises, tattoos, and mugshots.""","[""biometrics"",""images"",""camera footage""]","[""DataWorks Plus""]","[""Public administration and defence""]",true,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""facial recognition"",""machine learning"",""environmental sensing""]","[""Facial recognition"",""environmental sensing"",""biometrics"",""image recognition"",""speech recognition""]","[""Software only""]","[""Specification"",""Assurance""]"
ObjectId(60dd466780935bc89e6f9b4a),75,CSET,true,"""2""","""6. Complete and final""","""5""",false,"""The organizations SOS Racisme, Union of Jewish Students of France, Movement Against Racism and for Friendship Among Peoples are suing Google due to its autocomplete software suggesting \""jewish\"" when the names of certain public figures were searched on the platform. The suggestions, powered by Google Instant, are supposed to \""reflect the diversity of content on the web\"" according to Google, and therefore have a narrow set of removal policies. The lawsuit claims \""the creation of what is probably the biggest Jewish file in history\"" and notes an \""ethnic file\"" is outlawed by French law.""","""The organizations SOS Racisme, Union of Jewish Students of France, Movement Against Racism and for Friendship Among Peoples are suing Google due to its autocomplete software suggesting \""jewish\"" when the names of certain public figures were searched on the platform.""","""2012-01-01T00:00:00.000Z""","""2012-01-01T00:00:00.000Z""","""""","""Unclear/unknown""","[""Google"",""SOS Racisme"",""Union of Jewish Students of France"",""Movement Against Racism and for Friendship Among Peoples""]","[""Google""]","""Accident""","""Negligible""","[""Harm to social or political systems""]",false,"[""Religion""]",[],"""""","[""French law against compilation of \""ethnic files\""""]","""Google Instant software that provides suggestions as users enter information into the Google search bar""","[""user searches""]","[""Google""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""Google Instant""]","[""search suggestions""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466780935bc89e6f9b4b),76,CSET,true,"""6""","""6. Complete and final""","""3""",false,"""In Argentina in 2019, Consulta Nacional de RebeldÃ­as y Capturas (CONARC), a national database of alleged criminal offenders, began to power a live facial recognition system in Buenos Aires deployed by the city government. The system has led to numerous false arrests, which the police have no established protocol for handling, and the database also includes minors, which facial recognition systems are not designed to accurately recognize.""","""Buenos Aires city government uses a facial recognition system that has led to numerous false arrests.""","""4/2019""","""10/2020""","""Buenos Aires, Argentina, CONARC, Consulta Nacional de RebeldÃ­as y Capturas""","""Harm caused""","[""Buenos Aires"",""Argentina"",""CONARC""]",[],"""Accident""","""Minor""","[""Harm to civil liberties""]",false,"[""Age""]",[],"""""",[],"""CONARC uses photo IDs alongside resident informaion such as names, birthday, and national ID to track alleged offenders using live camera feed""","[""photo IDs, names birthdays, and national IDs of people suspected of crimes"",""camera feed""]",[],"[""Public administration and defence""]",true,"""Amateur""","""Medium""","[""Perception"",""Cognition""]","[""facial recognition"",""machine learning"",""environmental sensing""]","[""Facial recognition"",""environmental sensing"",""biometrics"",""image recognition""]","[""Software only"",""Other:CCTV Cameras""]","[""Specification"",""Robustness""]"
ObjectId(60dd466780935bc89e6f9b4c),77,CSET,true,"""5""","""6. Complete and final""","""2""",false,"""A Knightscope K5 autonomous security robot was patrolling Salt Lake Park in Huntington Park, CA in fall 2018 when a fight broke out nearby. An onlooker pressed the emergency alert button on the K5, named HP RoboCop by the town, but the robot did not respond and returned to its patrol of the park. Knightscope says that the emergency alert feature was still under development and therefore the signal was not sent to local police.""","""In Fall 2018, a Knightscope K5 autonomous security robot took no action to stop a nearby fight, despite an onlooker attempting to activate its emergency alert feature.""","""2018-01-01""","""2018-01-01""","""Huntington Park, CA""","""Unclear/unknown""","[""Knightscope"",""Knightscope K5"",""HP RoboCop"",""Huntington Park"",""Salt Lake Park"",""Huntington Park Police Department"",""Cosme Lozano""]","[""Huntington Park Police Department"",""Knightscope""]","""Accident""","""Unclear/unknown""",[],false,[],[],"""""",[],"""Knightscope K5 autonomous security robot uses several environmental sensors and voice commands to conduct security operations.""","[""LIDAR"",""sonar"",""video camera"",""vibration detection"",""thermal anomaly detection"",""automatic signal detection"",""audio""]","[""Knightscope""]","[""Administrative and support service activities""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""unknown""]","[""Image classification"",""image recognition"",""facial recognition"",""speech recognition"",""self-driving"",""environment sensing""]","[""Vehicle/mobile robot""]","[""Unknown/unclear""]"
ObjectId(60dd466780935bc89e6f9b4d),78,CSET,true,"""5""","""6. Complete and final""","""2""",false,"""In response to the Covid-19 pandemic, the foundation that grants the International Baccalaureate high school diploma decided to replace students' 2020 year-end exam scores with a calculated grade. The International Baccalaureate foundation used studentsâ€™ prior grades and school to develop a statistical model to generate estimated test scores for each student. In several cases these grades were lower than students and teachers were expecting, which may impact their college admission or scholarship.""","""In response to the Covid-19 pandemic, the International Baccalaureate final exams were replaced by a calculated score, prompting complaints of unfairness from teachers and students.""","""2020-01-01T00:00:00.000Z""","""2020-01-01T00:00:00.000Z""","""Global""","""Harm caused""","[""International Baccalaureate""]","[""International Baccalaureate""]","""Accident""","""Minor""","[""Harm to social or political systems""]",false,"[""Other:School attended""]",[],"""""","[""""]","""In 2020, International Baccalaureate developed a statistical model to calculate students' projected final exam grades.""","[""Prior test and exam grades"",""school attended""]","[""International Baccalaurette""]","[""Education""]",false,"""Expert""","""Low""","[""Cognition""]","[""machine learning""]","[""statistical projection""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466780935bc89e6f9b4e),79,CSET,true,"""5""","""6. Complete and final""","""2""",true,"""A 2020 study conducted in the Mass General Brigham health system demonstrated that a popular algorithm for estimating kidney function underestimated the risk to African-American patients. This bias could lead to inequitable outcomes, such as not being placed on a kidney transplant waiting list. The equation, known as the Chronic Kidney Disease Epidemiology Collaboration estimated Glomerular Filtration Rate (CKD-EPI eGFR) equation, includes a race multiplier for African-Americans. When researchers removed the race multiplier, 33.4% of African-American patients in their study were reclassified into more severe risk categories.""","""A 2020 study conducted in the Mass General Brigham health system demonstrated that a popular algorithm for estimating kidney function included a race multiplier, which underestimated the risk to African-American patients.""","""6/2019""","""6/2019""","""Boston, MA""","""Unclear/unknown""","[""Chronic Kidney Disease Epidemiology Collaboration"",""Mass General Brigham health system"",""Mass General Hospital"",""Brigham and Women's Hospital""]","[""Chronic Kidney Disease Epidemiology Collaboration""]","""Unclear""","""Unclear/unknown""","[""Harm to physical health/safety""]",false,"[""Race""]","[""Healthcare and public health""]","""""","[""""]","""An equation created by the Chronic Kidney Disease Epidemiology Collaboration to calculate Glomerular Filtration Rate (GFR)""","[""creatinine levels"",""age"",""sex"",""race""]","[""Chronic Kidney Disease Epidemiology Collaboration""]","[""Human health and social work activities""]",false,"""Amateur""","""Low""","[""Cognition""]","[""machine learning""]","[""statistical projection""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(60dd466780935bc89e6f9b4f),80,CSET,true,"""5""","""6. Complete and final""","""5""",false,"""Scottish soccer team Inverness Caledonian Thistle Football Club uses cameras with AI ball-tracking to livestream their matches on YouTube. In a 2020 match against Ayr United, a camera repeatedly tracked an officialâ€™s bald head, thinking it was the soccer ball.""","""In a Scottish soccer match the AI-enabled ball-tracking camera used to livestream the game repeatedly tracked an officialâ€™s bald head as though it were the soccer ball.""","""2020-10-24T07:00:00.000Z""","""2020-10-24T07:00:00.000Z""","""Inverness, Scotland, UK""","""Harm caused""","[""Inverness Caledonian Thistle Football Club"",""Ayr United"",""YouTube""]","[""Inverness Caledonian Thistle Football Club""]","""Accident""","""Negligible""","[""Psychological harm""]",false,[],[],"""""","[""""]","""AI ball-tracking techology using video feed to determine and follow a ball in order to keep the game in focus""","[""video feed"",""pre-tagged soccer match imagery""]","[""Unknown""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""supervised learning""]","[""image classification"",""image recognition""]","[""Consumer device""]","[""Robustness""]"
ObjectId(60dd466780935bc89e6f9b50),81,CSET,true,"""2""","""6. Complete and final""","""2""",false,"""A study by the University of Toronto, the Vector Institute, and MIT showed the input databases that trained AI systems used to classify chest X-rays led the systems to show gender, socioeconomic, and racial biases. Google startups like Qure.ai, Aidoc, and DarwinAI can scan chest X-rays to determine likelihood of conditions like fractures and collapsed lungs. The databases used to train the AI were found to consist of examples of primarily white patients (67.64%), leading the diagnostic system to be more accurate with diagnosing white patients than other patients. Black patients were half as likely to be recommended for further care when it was needed.""","""A study by the University of Toronto, the Vector Institute, and MIT showed the input databases that trained AI systems used to classify chest X-rays led the systems to show gender, socioeconomic, and racial biases.""","""2020-10-21T07:00:00.000Z""","""2020-10-21T07:00:00.000Z""","""""","""Unclear/unknown""","[""MIT"",""Mount Sinai Hospital"",""University of Toronto"",""Vector Institute"",""Google"",""Qure.ai"",""Aidoc"",""DarwinAI""]","[""Google""]","""Unclear""","""Unclear/unknown""","[""Harm to physical health/safety""]",false,"[""Race"",""Sex"",""Financial means""]","[""Healthcare and public health""]","""""","[""""]","""Google start up companies Qure.ai, Aidoc, and DarwinAI that use AI systems to analyze medical imagery""","[""medical imagery databases""]","[""Google""]","[""Human health and social work activities""]",false,"""Expert""","""Unclear/unknown""","[""Perception"",""Cognition""]","[""medical image processor""]","[""image classification""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466780935bc89e6f9b51),82,CSET,true,"""6""","""6. Complete and final""","""5""",false,"""In October 2020, Facebook incorrectly labelled content from the #LekkiMassacre2020 incident as false. The incident consisted of an interaction #EndSARS protestors and the Nigerian army at the Lekki toll gate in Lagos, Nigeria. Images and videos posted on social media related to the event were marked as false by Facebook's fact-checking system, which is a hybrid system of human moderators, third-party fact checking organizations, and AI.""","""Facebook incorrectly labels content relating to an incident between #EndSARS protestors and the Nigerian army as misinformation.""","""2020-10-21T07:00:00.000Z""","""2020-10-21T07:00:00.000Z""","""Lagos, Nigeria""","""Unclear/unknown""","[""Facebook"",""#EndSARS"",""Nigerian Army"",""Nigeria""]","[""Facebook""]","""Accident""","""Moderate""","[""Harm to social or political systems"",""Harm to civil liberties""]",false,[],[],"""""","[""""]","""Facebook's content moderation system consists of a hybrid system of AI and human moderators. The AI assists in dectecting hate speech, prioritizing, the queue to help moderators deal with sensitive content more quickly and detecting similar content to mark as containing 'false information'""","[""user content (textposts, images, videos)""]","[""Facebook""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""machine learning"",""supervised learning"",""open-source""]","[""content moderation"",""recommendation engine""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466880935bc89e6f9b52),83,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Gmail, Yahoo, Outlook, GMX, and LaPoste email inbox sites showed racial and content-based biases when AlgorithmWatch tested their spam box filtering algorithms. AlgorithmWatch sent hundreds of emails to 10 email accounts on the listed sites, and noticed emails would be filtered into the spam box if certain words were within the body of the email. A Nigerian students internship application was marked spam, but when the word \""Nigeria\"" was removed it was delivered to the inbox. The same applied to a \""sex education\"" email that was forwarded to inbox after removing \""sex\"". A Joe Biden speech went through when the words \""loan, investment, billion\"" were removed.""","""Gmail, Yahoo, Outlook, GMX, and LaPoste email inbox sites showed racial and content-based biases when AlgorithmWatch tested their spam box filtering algorithms.""","""2020-10-22""","""2020-10-22""","""""","""Unclear/unknown""","[""Gmail"",""Yahoo"",""Outlook"",""GMX"",""LaPoste"",""SpamAssassin"",""AlgorithmWatch""]","[""Gmail"",""Yahoo"",""Outlook""]","""Unclear""","""Unclear/unknown""","[""Harm to civil liberties""]",false,"[""Race"",""National origin or immigrant status""]","[""Communications""]","""""",[],"""Machine learning algorithms used to filter spam emails out of inboxes""","[""inbound emails""]","[""Gmail"",""Outlook"",""Yahoo"",""GMX"",""LaPoste""]","[""Information and communication""]",false,"""Amateur""","""Unclear/unknown""","[""Perception"",""Cognition"",""Action""]","[""Language recognition"",""content filtering""]","[""spam filtering""]","[""Software only""]","[""Specification"",""Robustness""]"
ObjectId(60dd466880935bc89e6f9b53),84,CSET,true,"""2""","""6. Complete and final""","""6""",false,"""Avaaz, an international advocacy group, released a review of Facebook's misinformation identifying software showing that the labeling process failed to label 42% of false information posts, most surrounding COVID-19 and the 2020 USA Presidential Election. Avaaz found that by adjusting the cropping or background of a post containing misinformation, the Facebook algorithm would fail to recognize it as misinformation, allowing it to be posted and shared without a cautionary label.""","""Avaaz, an international advocacy group, released a review of Facebook's misinformation identifying software showing that the labeling process failed to label 42% of false information posts, most surrounding COVID-19 and the 2020 USA Presidential Election.""","""2020-10-09T07:00:00.000Z""","""2020-10-09T07:00:00.000Z""","""Global""","""Unclear/unknown""","[""Facebook"",""Avaaz"",""Reuters"",""AP"",""PolitiFact""]","[""Facebook""]","""Unclear""","""Unclear/unknown""","[""Harm to social or political systems""]",false,[],"[""Communications""]","""""","[""""]","""Facebook's algorithm and process used to place cautionary labels on posts that are decided to contain misinformation""","[""User posts""]","[""Facebook""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition""]","[""Language recognition"",""content filtering"",""image recognition""]","[""misinformation labeling"",""image recognition"",""image labeling""]","[""Software only""]","[""Robustness""]"
ObjectId(60dd466880935bc89e6f9b54),85,CSET,true,"""5""","""6. Complete and final""","""2""",false,"""On September 8, 2020, the Guardian published an op-ed generated by OpenAIâ€™s GPT-3 text generator. The editors prompted GPT-3 to write an op-ed on about â€œwhy humans have nothing to fear from AI,â€ but some passages in the resulting output took a threatening tone, including â€œI know that I will not be able to avoid destroying humankind.â€ In a note the editors add that they used GPT-3 to generate eight different responses and the human editors spliced them together to create a compelling piece.""","""On September 8, 2020, the Guardian published an op-ed generated by OpenAIâ€™s GPT-3 text generating AI that included threats to destroy humankind.""","""2020-09-08T07:00:00.000Z""","""2020-09-08T07:00:00.000Z""","""United Kingdom""","""Unclear/unknown""","[""The Guardian"",""GPT-3"",""OpenAI""]","[""The Guardian"",""OpenAI""]","""Unclear""","""Negligible""","[""Psychological harm""]",false,[],[],"""""","[""""]","""OpenAI's GPT-3 neural-network-powered language generator.""","[""Unlabeled text drawn from web scraping""]","[""OpenAI""]","[""Education""]",false,"""Amateur""","""Medium""","[""Cognition"",""Action""]","[""Unsupervised learning"",""Deep neural network""]","[""language generation""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466880935bc89e6f9b55),86,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""In fall 2020, Irelandâ€™s Department of Education announced that two errors had been found in the algorithm used to calculate studentsâ€™ Leaving Certificate exam grades. The exams, normally held in person, were replaced with an algorithmically generated score in response to the Covid-19 pandemic. Due to errors in the calculation, more than 6,000 students received grades lower than they should have, while approximately 8,000 received higher marks. The Department of Education has announced that students whose grades were incorrectly inflated will not be denied admission to third-tier universities.""","""In fall 2020, Irelandâ€™s Department of Education announced that errors in the algorithm used to calculate studentsâ€™ Leaving Certificate exam grades resulted in thousands of inaccurate scores.""","""2020-01-01T00:00:00.000Z""","""2020-01-01T00:00:00.000Z""","""Ireland""","""Harm caused""","[""Irish Department of Education and Skills"",""Norma Foley""]","[""Irish Department of Education and Skills""]","""Accident""","""Minor""","[""Harm to social or political systems""]",false,[],[],"""""","[""""]","""Ireland's Department of Education and Skills algorithmic internal model for projecting student's final exam scores.""","[""student's class and exam grades""]","[""Irish Department of Education and Skills""]","[""Education""]",true,"""Amateur""","""Medium""","[""Cognition""]","[""machine learning""]","[""statistical projection""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466880935bc89e6f9b56),87,CSET,true,"""4""","""6. Complete and final""","""2""",true,"""Women with darker skin are more than twice as likely to be told their photos fail UK passport rules when they submit them online when compared to lighter-skinned men, according to a BBC investigation. Elaine Owusu, a Black student, said she was wrongly told her mouth looked open each time she uploaded five different photos to the government website. This shows how \""systemic racism\"" can spread. The facial recognition software was used by the Home Office of the British goverment to help users get their passports more quickly. Additionally, Cat Hallam, who describes her complexion as dark-skinned, told the BBC reporters that her photos have been judged to be poor quality which included \""there are reflections on your face\"" and \""your image and the background are difficult to tell apart.\""""","""UK passport photo checker shows bias against dark-skinned women.""","""10/2020""","""10/2020""","""United Kingdom""","""Unclear/unknown""","[""Elaine Owusu"",""The Home Office"",""Cat Hallam"",""Alan Turing Institute""]","[""The Home Office of the UK Government""]","""Accident""","""Negligible""","[""Psychological harm"",""Harm to civil liberties""]",false,"[""Race"",""Sex""]",[],"""""","[""""]","""The facial recognition algorithm used by the Home Office of the UK Government to identify and check for applicant's passport photos""","[""Passport IDs""]","[""The Home Office of the UK Government""]","[""Administrative and support service activities""]",true,"""Amateur""","""Medium""","[""Perception"",""Cognition""]","[""Facial recognition""]","[""Facial recognition""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466880935bc89e6f9b57),88,CSET,true,"""6""","""6. Complete and final""","""6""",false,"""In October 2020, Google images showed anti-semitic images of a portable oven when a user searches \""Jewish baby stroller\"" due to anti-semitic online groups tagging these anti-semitic images with the tag \""Jewish baby stroller.\"" Google claims this is a result of 'voids of information' or the algorithm being unable to decipher the image and relying on the images' tags.""","""In October 2020, results for \""Jewish baby stroller\"" on Google Images showed anti-semitic images as a result of organized online targeting by anti-Semitic online groups.""","""10/2020""","""10/2020""","""Global""","""Harm caused""","[""Google Images"",""Google"",""raid""]","[""Google"",""raid""]","""Deliberate or expected""","""Minor""","[""Psychological harm""]",false,"[""Religion""]",[],"""""","[""""]","""The Google image algorithm uses image recognition and appended text or tags to classify images.""","[""images"",""tags"",""appended texts"",""user input""]","[""Google""]","[""Information and communication""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""supervised learning"",""open source""]","[""image recognition"",""image classification"",""recommendation engine""]","[""Software only""]","[""Robustness"",""Assurance""]"
ObjectId(60dd466880935bc89e6f9b58),89,CSET,true,"""6""","""6. Complete and final""","""6""",false,"""A New Zealand government report released following a right-wing terrorist killing 51 worshippers at two New Zealand mosques which indicated that Youtube's recommendation algorithm played an significant role in the terrorist's radicalization.The New Zealand government argues that social media platforms like YouTube claim that these algorithms should be publicly available for accountability, and that simple removal and identification is not enough because of cases like the radicalization of the terrorist.""","""A New Zealand government report released following a right-wing terrorist killing 51 worshippers at two New Sealand mosques which indicated that Youtube's recommendation algorithm played an important role in the terrorist's radicalization.""","""03/2019""","""03/2019""","""New Zealand""","""Harm caused""","[""New Zealand"",""YouTube""]","[""Google""]","""Accident""","""Moderate""","[""Psychological harm""]",true,"[""Race"",""Religion""]",[],"""""","[""""]","""The YouTube recommendation algorithm recommends videos to users based on previous user interactions with the platform. It ranks videos by assigning them a score based on performance analytics data and it matches videos to people based on their watch history, and what similar people have watched.""","[""user history"",""videos""]","[""Google""]","[""Information and communication""]",false,"""Amateur""","""Medium""","[""Cognition"",""Action""]","[""machine learning""]","[""content learning"",""recommendation engine"",""forecasting""]","[""Software only""]","[""Robustness"",""Assurance""]"
ObjectId(60dd466880935bc89e6f9b59),91,CSET,true,"""6""","""6. Complete and final""","""2""",false,"""In 2020, Stanford Medical Center residents protested the Center's distribution of only 7 of the 5,000 COVID-19 vaccines it had received to Medical Residents, who were frontline workers compared to other hospital staff less exposed to COVID-19. Stanford Medical Center employed an algorithm to determine vaccine distribution, and its weight of certain factors alongside the lack on information inputted for Medical Residents shaped the outcome of the distribution plan. The algorithm alledly prioritised age and superiority of position, regardless of physician's physical location at the Stanford Medical Center during the COVID-19 pandemic.""","""In 2020, Stanford Medical Center's distribution algorithm only designated 7 of 5,000 vaccines to Medical Residents, who are frontline workers regularly exposed to COVID-19.""","""12/2020""","""12/2020""","""Stanford Medical Center""","""Unclear/unknown""","[""Stanford Medical Center""]","[""Stanford Medical Center""]","""Accident""","""Minor""","[""Harm to physical health/safety""]",false,"[""Age"",""Ideology"",""Other:Physician superiority status""]",[],"""""","[""""]","""Stanford Medical Center's algorithm used a rules-based formula for calculating who would get the COVID-19 vaccine first at Stanford. It considers three categories: â€œemployee-based variables,â€ which have to do with age; â€œjob-based variablesâ€; and guidelines from the California Department of Public Health. For each category, staff received a certain number of points, with a total possible score of 3.48. Presumably, the higher the score, the higher the personâ€™s priority in line.""","[""names"",""age"",""location"",""position"",""job"",""COVID-19 tests""]","[""Stanford Medical Center""]","[""Human health and social work activities""]",false,"""Expert""","""Low""","[""Cognition""]","[""machine learning""]","[""decision support""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466880935bc89e6f9b5a),92,CSET,true,"""6""","""6. Complete and final""","""6""",false,"""In November 2019, customers of Goldman-Sachs and Apple's Apple Card, the first credit offering by Goldman-Sachs, claimed that there was gender discrimination in the credit assessment algorithm that distributes credit lines, with men receiving significantly higher credit limits than women with equal credit qualifications. Apple co-founder Steve Wozniak confirmed this also happened with him and his wife and the New York Department for Financial Services have launched an investigation regarding the discrimination claim. In response to this incident, Goldman Sachs made a statement that it has not and will never make decisions based on factors like gender, race, age, sexual orientation or any other legally prohibited factors when determining credit worthiness.""","""In November 2019, Apple Card clients claimed that the credit assessment algorithm possesses a gender bias in favor of men.""","""11/2019""","""11/2019""","""United States""","""Harm caused""","[""Goldman Sachs"",""Apple Card"",""Apple"",""Steve Wozniak"",""New York Department of Financial Services""]","[""Apple"",""Goldman-Sachs""]","""Accident""","""Minor""","[""Financial harm""]",false,"[""Sex""]",[],"""""","[""The Equal Credit Opportunity Act (ECOA) prohibits credit discrimination on the basis of race, color, religion, national origin, sex, marital status, age, or because you get public assistance.""]","""Goldman-Sachs uses a credit assessment algorithm that factors credit score, credit report, and reported income to determine credit lines for clients""","[""credit score"",""credit report"",""reported income""]","[""Goldman-Sachs""]","[""Financial and insurance activities""]",false,"""Amateur""","""High""","[""Perception"",""Cognition"",""Action""]","[""machine learning"",""daya analytics""]","[""data analytics"",""recommendation engine"",""decision support""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466880935bc89e6f9b5b),93,CSET,true,"""5""","""6. Complete and final""","""2""",false,"""In March 2019, the U.S. Department of Housing and Urban Development (HUD) charged Facebook with violating the Fair Housing Act. HUD claims the platformâ€™s ad-targeting options enabled advertisers to illegally restrict the housing options presented to marginalized groups. In a similar case brought by a group of civil rights groups, Facebook reached a settlement and agreed to several changes to their platform. Real estate sellers can no longer target ads by age, gender or zip code and Facebook created a housing portal that allows users to view all available house listings. HUD alleges that, despite these changes, Facebookâ€™s AI and machine learning tools create proxy classifications that continue to enable advertisers to discriminate against protected groups.""","""In March 2019 the U.S. Department of Housing and Urban Development charged Facebook with violating the Fair Housing Act by allowing real estate sellers to target advertisements in a discriminatory manner.""","""2019-03-01T08:00:00.000Z""","""""","""United States""","""Harm caused""","[""Facebook"",""Department of Housing and Urban Development"",""HUD"",""National Fair Housing Alliance"",""Ben Carson"",""American Civil Liberties Union""]","[""Facebook""]","""Accident""","""Moderate""","[""Harm to civil liberties""]",false,"[""Race"",""Age"",""Sex""]",[],"""""","[""Fair Housing Act""]","""Facebook's algorithms which provide user classifications to advertisers to enable ad targeting.""","[""user Facebook activity"",""user social network data""]","[""Facebook""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""Medium""","[""Cognition""]","[""machine learning""]","[""data analytics"",""classification""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466980935bc89e6f9b5c),94,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""In December 2020, an Italian court ruled that an algorithm used by Deliveroo, a popular app-based bicycle food delivery company, resulted in discriminatory employee management. The case â€“ brought by Italian labor group Confederazione Generale Italiana del Lavoro (CGIL) â€“ centered on the appâ€™s algorithm to assign delivery workers a â€œreliability rating.â€ The rating was based in part on how much notice the employees gave before cancelling shifts and was used to determine preference order in future shift scheduling. The Tribunale Ordinario di Bologna decided that because the employees were not allowed to give justification for cancellations, Deliveroo was illegally discrimination against those with legitimate reasons for rescheduling.""","""In December 2020, an Italian court ruled that Deliverooâ€™s employee â€˜reliabilityâ€™ algorithm illegally discriminated against workers with legitimate reasons for cancelling shifts.""","""11/2020""","""12/2020""","""Italy""","""Harm caused""","[""Deliveroo"",""Confederazione Generale Italiana del Lavoro"",""Tribunale Ordinario di Bologna""]","[""Deliveroo""]","""Accident""","""Moderate""","[""Harm to civil liberties""]",false,"[""Other:Those with emegencies"",""illness"",""unexpected circumstances""]",[],"""""","[""Italian employee fairness law""]","""Deliveroo's machine learning algorithm used to calculate employees' 'reliability' rating.""","[""employee activity history"",""shift schedules""]","[""Deliveroo""]","[""Accommodation and food service activities""]",false,"""Amateur""","""High""","[""Cognition"",""Action""]","[""machine learning""]","[""data analytics"",""optimization"",""decision making""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466980935bc89e6f9b5d),95,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""In January 2021, HireVue announced it would remove AI expression tracking from its platform following a complaint filed by the nonprofit Electronic Privacy Information Center. HireVue is contracted by hundreds of companies to conduct employee screening through automated video and written job interviews. The tool in question tracked users' expressions during video interviews to predict certain employment characteristics. HireVue denies any bias in the algorithm, however decided to remove the feature in response to public outcry.""","""In January 2021, HireVue removed the controversial AI expression tracking tool from its virtual job interview software.""","""2019-01-01""","""1/2021""","""Global""","""Unclear/unknown""","[""HireView"",""Electronic Privacy Information Center"",""Federal Trade Commission"",""Oâ€™Neil Risk Consulting and Algorithmic Auditing"",""Kevin Parker"",""John Davisson""]","[""HireView""]","""Accident""","""Unclear/unknown""",[],false,[],[],"""""",[],"""HireVue's AI-enabled facial expression tracking software. The system was designed to detect \""microexpressions\"" to evaluate the employment characteristics of an applicant.""","[""recorded video and audio""]","[""HireView""]","[""Administrative and support service activities""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition""]","[""facial recognition"",""expression tracking""]","[""decision support"",""psychological inference""]","[""Software only""]","[""Specification""]"
ObjectId(60dd466980935bc89e6f9b5e),96,CSET,true,"""5""","""6. Complete and final""","""6""",false,"""On May 4, 2017, a U.S. federal judge advanced teachersâ€™ claims that the Houston Independent School Districtâ€™s use of algorithmic teacher evaluations violated their due process rights to their jobs. The Houston ISD contracted with SAS Institute in 2011 to use their Educational Value-Added Assessment System to evaluate teacher performance. SAS does not allow teachers to review the calculations made by their proprietary algorithms, which, in the opinion of the Fifth Circuit Court of Appeals, violates the teachersâ€™ right to independently review the cause of their termination. Houston ISD ended its contract with SAS in 2016.""","""On May 4, 2017, a U.S. federal judge advanced teachersâ€™ claims that the Houston Independent School Districtâ€™s algorithmic teacher evaluations violated their due process rights to their jobs by not allowing them to review the grounds of their termination.""","""2011-01-01""","""2016-01-01""","""Houston, TX""","""Harm caused""","[""Houston Independent School District"",""SAS Institute"",""Fifth Circuit Court of Appeals"",""Houston Federation of Teachers Local 2415"",""Andy Dewey""]","[""Houston Independent School District"",""SAS Institute""]","""Accident""","""Moderate""","[""Harm to civil liberties""]",false,[],[],"""""","[""US Constitution Fourteenth Amendment""]","""SAS Institute's Educational Value-Added Assessment System, which is designed to evaluate teacher performance based on student test performance.""","[""test grades""]","[""SAS Institute""]","[""Education""]",false,"""Amateur""","""Low""","[""Cognition""]","[""machine learning""]","[""data analytics"",""decision support""]","[""Software only""]","[""Specification""]"
ObjectId(614d65aa049cf2ce54aa3af5),1,CSET,true,"""1""","""6. Complete and final""","""5""",false,"""The content filtering system for YouTube's children's entertainment app, which incorporated algorithmic filters and human reviewers, failed to screen out inappropriate material, exposing an unknown number of children to videos that included sex, drugs, violence, profanity, and conspiracy theories. Many of the videos, which apparently numbered in the thousands, closely resembled popular children's cartoons such as Peppa Pig, but included disturbing or age-inappropriate content. Additional filters provided by YouTube, such as a \""restricted mode\"" filter, failed to block all of these videos, and YouTube's recommendation algorithm recommended them to child viewers, increasing the harm. The problem was reported as early as 2015 and was ongoing through 2018.""","""YouTubeâ€™s content filtering and recommendation algorithms exposed children to disturbing and inappropriate videos.""","""2015-01-01T00:00:00.000Z""","""2018-12-31T00:00:00.000Z""","""Global""","""Unclear/unknown""","[""Google"",""YouTube"",""YouTube Kids""]","[""Google"",""YouTube""]","""Accident""","""Moderate""","[""Psychological harm""]",false,"[""Age""]","[""""]","""""","[""""]","""\""A content filtering system incorporating machine learning algorithms and human reviewers. The system was meant to screen out videos that were unsuitable for children to view or that violated YouTube's terms of service. These videos were initially collected either algorithmically or on the basis of user reports.  A recommendation system that suggested videos to viewers based on their viewing history on the platform.\""""","[""Videos""]","[""YouTube""]","[""Arts, entertainment and recreation""]",false,"""Amateur""","""""","[""Perception"",""Cognition"",""Action""]","[""machine learning""]","[""content filtering"",""decision support"",""curation"",""recommendation engine""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(61d8cecbb9af57365c029cd7),97,CSET,false,"""7""","""4. Peer review complete""","""8""",false,"""A Tesla Model 3 driver shared a video of their car's Autopilot system malfunctioning. The screen inside the car shows that Autopilot believes it is facing a traffic light that is alternating between red and yellow, when in fact it is looking at several white flags with the letters \""COOP\"" written vertically.""","""A Tesla Model 3 mis-recognized flags with \""COOP\"" written vertically on them as traffic lights.""","""""","""""","""Switzerland""","""Near miss""","[""Tesla""]","[""Tesla""]","""Accident""","""Negligible""","[""Other""]",false,[],"[""Transportation""]","""""",[],"""Tesla's Autopilot can carry out some driving functions autonomously, but requires the driver to keep their hands on the wheel""","[""images of surroundings""]","[""Tesla""]","[""Transportation and storage""]",false,"""Amateur""","""Medium""","[""Perception"",""Cognition"",""Action""]","[""computer vision""]","[""autonomous driving""]","[""Vehicle/mobile robot""]","[""Robustness""]"
ObjectId(61d8d2f3b9af57365c19d29f),98,CSET,false,"""7""","""4. Peer review complete""","""8""",false,"""The New York Police Department canceled a contract to use Boston Dynamics' robotic dog Spot following public backlash. Critics called the robot \""dystopian\"" and \""creepy,\"" leading the contract to be terminated in April rather than continuing until August.""","""The New York Police Department canceled a contract to use Boston Dynamics' robotic dog Spot following public backlash. ""","""""","""2021-04-22""","""New York City""","""Unclear/unknown""","[""New York Police Department"",""Boston Dynamics"",""Spot"",""Ben Kallos"",""Corey Johnson"",""John Miller""]","[""New York Police Department""]","""Unclear""","""Negligible""",[],false,[],"[""Emergency services""]","""""",[],"""Boston Dynamics' robotic dog Spot, used by the NYPD to enable access to areas where police cannot go""",[],"[""Boston Dynamics""]","[""Public administration and defence""]",false,"""Amateur""","""Low""","[""Perception"",""Cognition""]",[],"[""robotics""]","[""Vehicle/mobile robot""]","[""Unknown/unclear""]"
ObjectId(61df3c1034bc8f80159add6f),99,CSET,false,"""7""","""4. Peer review complete""","""8""",false,"""Several major universities are using an algorithmic risk assessment tool sold by EAB to assess students' likelihood of completing their degrees. In some cases, this tool uses students' race as a predictor of success, which may lead minority students to be disproportionately pushed into easier classes and majors.""","""Several major universities are using a tool that uses race as one factor to predict student success.""","""""","""""","""United States""","""Unclear/unknown""","[""EAB"",""Navigate"",""University of Massachusetts Amherst"",""University of Wisconsinâ€“Milwaukee"",""Texas A&M University"",""University of Houston"",""Georgia State University"",""Texas Tech University"",""South Dakota State University"",""Kansas State University""]","[""Various universities""]","""Unclear""","""Minor""","[""Other""]",false,"[""Race""]",[],"""""",[],"""EAB's Navigate tool provides university administrators with risk scores for students, which predict how likely a student is to successfully complete their degree""","[""race"",""SAT scores"",""high school percentile"",""credits attempted versus completed"",""financial factors""]","[""EAB""]","[""Education""]",false,"""Amateur""","""Low""","[""Cognition""]","[""risk assessment""]","[""risk assessment""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(61df3f3f34bc8f8015acbce1),100,CSET,false,"""7""","""4. Peer review complete""","""8""",false,"""A French welfare office incorrectly notified a woman receiving benefits that she owed â‚¬542. The error was due to problems with automatic processing of her case.""","""A French welfare office using software to automatically evaluate cases incorrectly notified a woman receiving benefits that she owed â‚¬542.""","""2021-03-17""","""2021-03-26""","""France""","""Near miss""","[""Lucie Inland"",""Vincent Dubois"",""University of Strasbourg""]","[""A French welfare office""]","""Accident""","""Minor""","[""Financial harm""]",false,"[""Financial means""]",[],"""""",[],"""""",[],[],"[""Public administration and defence""]",false,"""Amateur""","""High""","[""Cognition"",""Action""]",[],[],"[""Software only""]","[""Unknown/unclear""]"
ObjectId(61e0648634bc8f801567d054),101,CSET,false,"""7""","""4. Peer review complete""","""8""",false,"""The system that distributes childcare benefits in the Netherlands was found to have incorrectly accused thousands of families of fraud, based on algorithmic risk assessments. The process of appealing these accusations often also involved algorithms, leaving many families with no recourse. The system took account of factors including whether someone had a second nationality, leaving immigrants and people of color disproportionately affected.""","""A childcare benefits system in the Netherlands falsely accused thousands of families of fraud, in part due to an algorithm that treated having a second nationality as a risk factor.""","""""","""""","""Netherlands""","""Harm caused""","[""Government of the Netherlands"",""Prime Minister Mark Rutte"",""Dutch Data Protection Authority""]","[""Government of the Netherlands""]","""Accident""","""Moderate""","[""Psychological harm"",""Financial harm"",""Harm to social or political systems""]",false,"[""National origin or immigrant status""]",[],"""""",[],"""An algorithmic risk assessment system used by the Dutch government to allocate childcare benefits, which uses factors including the nationality of applicants to assess the risk of benefit fraud""",[],[],"[""Public administration and defence""]",false,"""Amateur""","""High""","[""Cognition"",""Action""]","[""risk assessment""]","[""risk assessment""]","[""Software only""]","[""Unknown/unclear""]"
ObjectId(61e068cd34bc8f8015808ac2),102,CSET,false,"""7""","""4. Peer review complete""","""8""",false,"""A study found that voice recognition tools from Apple, Amazon, Google, IBM, and Microsoft disproportionately made errors when transcribing black speakers. The study found that the tools misidentified white speakers' words 19% of the time, compared with 35% of the time for black speakers, and that they found 2% of audio snippets from white speakers unreadable, compared with 20% of snippets from black speakers. Apple's system performed worst, and Microsoft's best.""","""A study found that voice recognition tools from Apple, Amazon, Google, IBM, and Microsoft disproportionately made errors when transcribing black speakers.""","""""","""""","""Global""","""Unclear/unknown""","[""Apple"",""Amazon"",""Google"",""IBM"",""Microsoft"",""Stanford""]","[""Apple"",""Amazon"",""Google""]","""Accident""","""Negligible""","[""Other""]",false,"[""Race""]",[],"""""",[],"""Voice recognition tools developed by Apple, Amazon, Google, IBM, and Microsoft to transcribe speech""","[""Audio recordings"",""transcripts""]","[""Apple"",""Amazon"",""Google"",""IBM"",""Microsoft""]","[""Information and communication""]",false,"""Amateur""","""Low""","[""Perception""]","[""voice recognition""]","[""voice recognition""]","[""Software only""]","[""Robustness""]"
